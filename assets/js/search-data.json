{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://lucacazzanti.github.io/abacus-canvas/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lucacazzanti.github.io/abacus-canvas/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Confidence Intervals for Gamma Distribution Parameters",
            "content": "Summary . I use visualization and bootstrap to obtain empirical confidence intervals for the estimates of the $k$ parameter of a Gamma distribution. I consider the dependency of the estimates on the sample size and on the true value of $k$, and the differences in the estimates computed with the method-of-moment (MoM) and maximum likelihood estimation (MLE). The results are in line with what we already know from statistics: . The higher sample size, the higher the achievable confidence for a given tolerance on the $k$ estimates. | MLE gives (slightly) more robust estimates than MoM across values of $k$ and across sample sizes. | I also quantify (approximately) the relationship between the confidence level and the fractional error in the estimates for $k$: At the 95% confidence level, estimates for $k$ deviate from the true value by $ pm$28% (sample size=100), by $ pm$12% (sample size=500), and by $ pm$9% (sample size=1000). These results suggest a minimum sample size in the range 100-500 for reasonable estimates in practical settings, although the acceptable deviation from true $k$ depends on one&#39;s application. . Tools used: the seaborn statistical visualization module, scipy, pandas, in addition to matplotlib. . The Gamma distribution: the basics . The Gamma distribution depends on two parameters, $k$ and $ theta$, and looks like this: . import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns import pandas as pd from scipy.stats import gamma import numpy as np . . def make_gammas(ks, thetas, x): &quot;&quot;&quot; Make a dataframe of Gamma PDFs. Args: ks (1d array-like): values of k thetas (1d array-like): values of theta x (1d array-like): points at which to evaluate the Gamma pdf. Returns: Pandas dataframe where eah column contains the Gamma pdf evaluated at x for each pairwise combination of the parameters in ks and thetas. &quot;&quot;&quot; res = {} ordered_keys = [] for theta in sorted(thetas): for k in sorted(ks): name = &#39;$k$=&#39; + str(k) + &#39;; $ theta$=&#39; + str(theta) res[name] = gamma.pdf(x, k, scale=theta) ordered_keys.append(name) return pd.DataFrame(res)[ordered_keys] . . gamma_df_k = make_gammas([1,2,3,5,7,9], [2], np.linspace(0,25, num=100)) gamma_df_theta = make_gammas([2],[1,2,5,7,11,13], np.linspace(0,25, num=100)) fig, axarr = plt.subplots(1,2,figsize=(15,5), sharey=True) gamma_df_k.plot(ax=axarr[0], fontsize=16) axarr[0].legend(fontsize=14) gamma_df_theta.plot(ax=axarr[1], fontsize=16) axarr[1].legend(fontsize=14) plt.suptitle(&#39;Gamma distributions for various $k$ and $ theta$ values&#39;, fontsize=18) . . &lt;matplotlib.text.Text at 0x7f77c7214590&gt; . From the above plots we see that both $k$ and $ theta$ affect the look of the Gamma pdf. The Gamma pdf is skewed to the left, with a longer tail on the right side of the mode. For $k$=1, the Gamma pdf is simply an exponential pdf (maximum asymmetry) and for $k &gt; approx$9 the pdf approaches a Gaussian (symmetry). Lower values of $ theta$ squish the distribution horizontally toward the left; higher values stretch it and flatten it, but the overall profile is not affected. For these reasons $k$ is called the shape parameter and $ theta$ the scale parameter. . The parameters $k$ and $ theta$ are related to each other through the mean and variance of the Gamma distribution: $$ mu = k theta, $$ $$ sigma^2=k theta^2.$$ Thus, given the mean, we only need to estimate one parameter and then compute the other from the estimate. The scipy.stats.gamma.fit() implementation lets you choose which parameter to estimate and which one to constrain: I chose to estimate $k$ and derive $ theta$. Finally, since the Gamma pdf increasingly resembles a Gaussian for higher values of $k$, here I limit my exploration to $k lt 10$. For higher values a Gaussian might be a better model, or one could shift the data x down (to the left) so that min(x)=0 and assume lower values of $k$ for approximating the shifted pdf. Either way, I look only at $k lt10$. . Estimating k in practice and confidence intervals . In pratical settings, a limited number of samples are available, which introduces uncertainty about the estimated value of $k$. We know from probability theory that the uncertainty decreases as the number $n$ of available samples increases: in fact, the uncertainty is inversely proportional to $ sqrt{n}$. One can measure this effect empirically by repeatedly estimating $k$ many times for different sample sizes, and observing the resulting sampling distribution: this is called bootstrap, and it&#39;s the method I adopt here. . With the bootstrap, one can compute the confidence intervals around the estimates. Confidence intervals are simple plus-or-minus bounds associated with the estimated value and with a level of confidence, which is simply the probability that the estimated $k$ will fall within the bounds. Typically folks aim for a 95% confidence interval: what are the bounds within which the true value of $k$ will fall with a probability of 95%? Or, you can turn around the question and ask: Given that I can only tolerate a given bound on the estimation error, what is the probability (confidence) that the estimate will fall within those bounds? . But how do we estimate $k$? There are two gneral approaches from statistical learning that we can apply here: the MoM and MLE. . MoM: Method of moments . With MoM, one sets the empirical moments (mean, variance etc.) to their theoretical expression in terms of the distribution parameters, then solves for the parameters. For a Gamma distribution we know that the mean $ mu=k theta$ and the variance $ sigma^2=k theta^2$. So, taking the values of $ mu$ and $ sigma^2$ from the sample, the closed-form solution for $k$ and $ theta$ is: $$k = frac{ mu^2}{ sigma^2},$$ $$ theta = frac{ sigma^2}{ mu}$$ . Here&#39;s a trivial Python implementation: . import numpy as np def gamma_mom(x): &quot;&quot;&quot; Estimate the parameters of a Gamma distribution using the method-of-moments. Args: x (1d array-like): Data, assumed drawn from a Gamma distribution. Returns: (k, theta) parameters &quot;&quot;&quot; avg = np.mean(x) var = np.var(x) k = avg**2 / var theta = var / avg return k, theta . MLE: Maximum likelihood estimation . With MLE, one maximizes the likelihood function of the observed the data over the Gamma parameters. For Gamma, there is no closed-form expression for the maximum as a function of the parameters, so we must resort to numerical methods. Luckily scipy.stats.gamma.fit() implements MLE for Gamma distributions for us, based on work by Choi and Wette and Minka. . MoM and MLE: single $k$ . Lets calibrate the discussion to what we already know. For a single value of $k$, let&#39;s generate many bootstrap draws the corresponding gamma and compare the sampling distributions for the estimates obtained with the MoM and the MLE. . import itertools def gamma_bootstrap_estimate(true_k, true_theta, sample_size=[50, 100, 1000], draws=100, method=&#39;all&#39;): &quot;&quot;&quot; Estimate the parameters of a Gamma distribution with bootstrap. Generates bootstrap samples and estimates the (k, theta) parameters from each draw, using the method-of-moments and scipy.stats.gamma.fit Args: true_k (scalar): the k parameter of the Gamma distribution. true_theta (scalar): the theta parameter of the Gamma distribution. sample_size (1d array-like of ints): the number of random samples to draw from the Gamma distribution for each bootstrap draw. draws (int): number of bootstrap draws (100) method (str): one of &#39;mom&#39;, &#39;scipy&#39;, or &#39;all&#39; (&#39;all&#39;) Returns: A pandas DataFrame where each row contains the estimates for k and theta for one bootstrap draw. &quot;&quot;&quot; true_mean = true_k * true_theta true_var = true_k * true_theta ** 2 result = [] for this_many in sample_size: # Generate this_many samples from the true Gamma rvs = [ gamma.rvs(true_k, scale=true_theta, size=this_many) for n in xrange(draws) ] if method == &#39;all&#39; or method == &#39;scipy&#39;: estimates_scipy = ( gamma.fit(x,floc=0) for x in rvs ) (k_scipy, loc_scipy, theta_scipy) = itertools.izip(*estimates_scipy) result.append({&#39;sample_size&#39;: this_many, &#39;k_estimate&#39;: k_scipy, &#39;theta_estimate&#39;: theta_scipy, &#39;true_theta&#39;: true_theta, &#39;true_k&#39;: true_k, &#39;method&#39;: &#39;scipy&#39;}) if method == &#39;all&#39; or method == &#39;mom&#39;: estimates_mom = (gamma_mom(x) for x in rvs) (k_mom, theta_mom) = itertools.izip(*estimates_mom) result.append({&#39;sample_size&#39;: this_many, &#39;k_estimate&#39;: k_mom, &#39;theta_estimate&#39;: theta_mom, &#39;true_theta&#39;: true_theta, &#39;true_k&#39;: true_k, &#39;method&#39;: &#39;mom&#39;}) return pd.concat( [pd.DataFrame(r) for r in result]) . true_k = 2 true_theta = 2 num_samples = [10, 25, 50, 100, 500, 1000] num_draws = 1000 estimates_for_one_k = gamma_bootstrap_estimate(true_k, true_theta, num_samples, draws=num_draws) estimates_for_one_k.head() . k_estimate method sample_size theta_estimate true_k true_theta . 0 3.465786 | scipy | 10 | 0.882095 | 2 | 2 | . 1 5.106975 | scipy | 10 | 0.797050 | 2 | 2 | . 2 3.020494 | scipy | 10 | 1.002957 | 2 | 2 | . 3 4.314278 | scipy | 10 | 1.233048 | 2 | 2 | . 4 2.175172 | scipy | 10 | 1.090711 | 2 | 2 | . The violin plot is a graphical data analysis method for direct comparison of two distributions across different values of a grouping variable (number of samples in our case). Let&#39;s take a look: . import seaborn as sns sns.set(style=&quot;ticks&quot;) sns.set_context(&quot;poster&quot;) # this helps when converting to static html for blog plt.figure(figsize=(15,7)) ax = sns.violinplot(x=&#39;sample_size&#39;, y=&#39;k_estimate&#39;, data=estimates_for_one_k, hue=&#39;method&#39;, palette=&#39;muted&#39;, inner=&#39;quartile&#39;, split=True, hue_order=[&#39;mom&#39;, &#39;scipy&#39;], linewidth=1) sns.despine(offset=10, trim=True) title_str = &#39;Estimates of k from &#39; + str(num_draws) + &#39; bootstrap draws; true k=&#39; + str(true_k) + &#39;, true $ theta=$&#39; + str(true_theta) plt.title(title_str) . &lt;matplotlib.text.Text at 0x7f77c717e210&gt; . Observations on the violin plot . I see that for both MoM and MLE the sampling distribution spread decreases as the number of samples increases. We knew his from the central limit theorem (CLM). I also see that the spread of the sampling distribution for the MoM estimates is a bit wider than for the MLE, which suggests that MLE could buy you a bit more confidence in the estimates. Next I want to look at these differences in spread more closely and test these observations for different values of $k$. . MoM and MLE: many $k$s . To compare the estimates across $k$, I normalize the error to a fraction between -1 and 1. I stop at k=9 because for higher values the Gamma distribution starts to look like a Gaussian, and a Gaussian model might be sufficient in practical settings. I visualize the results in panels of boxplots, which help hone in on the quartiles of a distribution. . df_list = [] theta_val = 2 for k in [1,2,3,5,7,9]: tmp = gamma_bootstrap_estimate(k,theta_val, sample_size=num_samples, draws=num_draws) df_list.append(tmp) big_df = pd.concat(df_list) big_df[&#39;fractional_error&#39;] = (big_df[&#39;k_estimate&#39;] - big_df[&#39;true_k&#39;] ) / big_df[&#39;true_k&#39;] big_df.head() . k_estimate method sample_size theta_estimate true_k true_theta fractional_error . 0 1.229787 | scipy | 10 | 1.059610 | 1 | 2 | 0.229787 | . 1 0.790053 | scipy | 10 | 3.453050 | 1 | 2 | -0.209947 | . 2 0.975968 | scipy | 10 | 1.461941 | 1 | 2 | -0.024032 | . 3 1.004771 | scipy | 10 | 1.284221 | 1 | 2 | 0.004771 | . 4 0.754723 | scipy | 10 | 1.668499 | 1 | 2 | -0.245277 | . # make and adjust the subplots manually... true_k = big_df[&#39;true_k&#39;].unique() num_k = len(true_k) ncol=3 nrow= num_k / ncol sns.set(style=&quot;ticks&quot;) sns.set_context(&quot;poster&quot;) f, axarr = plt.subplots(nrow, ncol, sharex=True, sharey=True, figsize=(15,14)) for row in range(nrow): for col in range(ncol): idx = row * ncol + col this_ax = axarr[row,col] sns.boxplot(ax=this_ax, x=&quot;sample_size&quot;, y=&quot;fractional_error&quot;, hue=&#39;method&#39;, hue_order=[&#39;mom&#39;, &#39;scipy&#39;], data=big_df[big_df[&#39;true_k&#39;] == true_k[idx] ], palette=&quot;muted&quot;, showfliers=False, linewidth=1, showmeans=True, meanline=True) this_ax.set_title(&#39;k=&#39;+str(true_k[idx])) if row == 0: this_ax.set_xlabel(&#39;&#39;) if col &gt; 0: this_ax.set_ylabel(&#39;&#39;) sns.despine(offset=10, trim=True) plt.subplots_adjust(wspace=0.4, hspace=0.3) plt.suptitle(&#39;Fractional estimation error across k, for $ theta$=&#39;+str(theta_val)) . &lt;matplotlib.text.Text at 0x7f77c459d0d0&gt; . Observations on the box plots . In the above grid of box plots I notice the following: . It seems that the MLE and MoM are not dependent on the true value of $k$, except that for $k=1$, for which the Gamma becomes an exponetial pdf, the MoM gives visually noticeable higher error. | MLE gives more robust estimates, which show up as smaller inter-quantile ranges (IQRs, the upper and lower box edges) | The median and mean fractional error (black and red lines, respectively, inside the boxes) for both MLE and MoM are consistently greater than 0, which tells me that the sampling distribution is skewed. This effect is particularly noticeable for sample sizes 10 and 25. Mean and median converge toward each other and toward 0 for larger sample sizes, which means that the sampling distributions become more symmetric. We observed this in the violin plots above, too, and it is expected by the central limit theorem (CLT). | Confidence intervals . I want to eastimate confidence intervals, so I compute quantiles of the estimates of $k$. Again, to facilitate the comparison across different true values of $k$ and across sample sizes I adopt the fractional error metric, but I take its absolute value to simplify the visual exploration of the charts below. In this case taking the absolute value is like assuming that the fractional error is symmetric around 0. We have seen above that this is not the case for small sample sizes, but for sample size 100 and greater it is more plausible. Nonetheless, for the purpose of this blog, please allow me to simplify in this way. . Finally, to avoid visual information overload, I do not color-code the different values of $k$ in the quantile plots below. Instead, estimates for different $k$s are color-coded by estimation method, so the plots will display 6 markers per method per confidence level, color-coded by MoM or MLE. This visualization is clearer and consistent with the observation that the true value of $k$ has a smaller effect on the estimates than the sample size. Here we go! . q = [0.05,0.1,0.2,0.3, 0.4,0.5,0.6,0.7,0.8,0.9, 0.95, 0.975] #quantiles big_df[&#39;abs_fractional_error&#39;] = big_df[&#39;fractional_error&#39;].abs() grouped = big_df.groupby([&#39;method&#39;, &#39;sample_size&#39;, &#39;true_k&#39;]) grouped_quantiles = grouped[&#39;abs_fractional_error&#39;].quantile(q) grouped_quantiles.name=&#39;fractional_error_quantile&#39; # Give a name to the quantile level column grouped_quantiles.index.names = map(lambda n: n if n is not None else &#39;confidence_level&#39;, grouped_quantiles.index.names) # Flatten the results for easier plotting: Took me a while poking around # the pandas doc to find this way to flatten the multi-index created by groupby() above! quantiles_df = pd.DataFrame( pd.DataFrame(grouped_quantiles).to_records() ) quantiles_df.head() . method sample_size true_k confidence_level fractional_error_quantile . 0 mom | 10 | 1 | 0.05 | 0.034163 | . 1 mom | 10 | 1 | 0.10 | 0.062888 | . 2 mom | 10 | 1 | 0.20 | 0.132973 | . 3 mom | 10 | 1 | 0.30 | 0.211379 | . 4 mom | 10 | 1 | 0.40 | 0.312867 | . sns.set(style=&quot;darkgrid&quot;) sns.set_context(&quot;poster&quot;) f = plt.figure(figsize=(17,15)) g = sns.FacetGrid(quantiles_df, despine=True, sharey=False, col_wrap=3, col=&#39;sample_size&#39;, size=4, legend_out=True, hue=&#39;method&#39;, hue_order=[&#39;mom&#39;, &#39;scipy&#39;], margin_titles=True, palette=&#39;muted&#39;, xlim=[-0.1, 1.1]) g.map(plt.scatter, &quot;confidence_level&quot;, &quot;fractional_error_quantile&quot;) # Adjust ylims and plot vertical line at a given confidence level conf_level = 0.95 for ax in g.axes.flat: ylims = ax.get_ylim() new_ylims = [-0.01, ylims[1]] ax.set_ylim(new_ylims) ax.plot([conf_level, conf_level], new_ylims, &#39;r--&#39;, alpha=0.5, linewidth=1) sns.despine(offset=10, trim=True) g.add_legend() g.fig.subplots_adjust(wspace=0.4, hspace=0.5); . &lt;matplotlib.figure.Figure at 0x7f77c4203050&gt; . Observations on the quantile plots . Our previous observation that MLE gives more robust estimates than MoM across sample sizes and values of $k$ is confirmed: MLE depends less on the value of $k$ (tighter green vertical spreads) and, for equal confidence level, gives less fractional error in the estimated $k$ (green plots consistently lower than blue plots). | For sample sizes &lt;=50, at a 95% confidence level (red vertical line) you&#39;d need to tolerate uncertainties in $k$ between about $ pm$ 40% to $ pm$ 200%. Perhaps your application can deal with this range, perhaps not. | For sample sizes &gt;= 100, the confidence intervals become more reasonable: for 500 samples the 95% confidence interval is about $ pm$ 12% for MLE, and about $ pm9$% for 1000 samples. | Conclusions . I explored the link between sampling size, Gamma distribution parameters, and estimation methods using the bootstrap and visualization. What I found is well known, not just for Gamma distributions: sampling effects diminish as the sample size increases and different estimation methods give different results. I was also interested in calculating confidence intervals on the estimates that I could use a guidelines in the future. I now know, for example, that if I want my estimate of $k$ to be within $ pm$15% of true value 95% of the times, I need about 500 samples. In fact, I suggest two guidelines: . Prefer MLE over MoM for Gamma distributions because it buys you a bit more confidence. | Prefer sample sizes &gt; 100, but 1000, or even 500 may not be necessary, depending on the requirements of your application. | Relevance: why does it matter? . Sampling introduces uncertainty in addition to the randomness of your chosen probability model. You should have an idea about this additional uncertainty, because it affects the predictions downstream of the estimation and therefore your key performance indicators (KPIs). In a typical data product (recommendation engine, personalized marketing offer, etc.), you have an overall error budget dictated by the allowed tolerances of your problem, and you allocate that error budget to the various steps in your data processing pipeline. For example, say your KPI is average revenue per user (ARPU) and you must estimate it within $ pm5%$, then all steps in your pipeline must guarantee that you will stay within that limit. . It&#39;s true that big data give us the luxury of huge sample sizes, compressing the uncertainty down to unreasonably effective ranges, and making sampling effects essentially insignificant. But even when you have big data sets available, you may need to partition it into many, disjoint subsets to run hundreds or thousands of independent experiments, each one requiring some sort of parameter estimation. This is the case of mass personalization, for example, in which finely targeted marketing offers are presented to smallish groups of customers. Determining the appropirate offer requires running many A/B tests, which means reserving target and control groups for each test. If each one requires 100 customers and you have 100 offers to test, then you need 10,000 customers set aside. If you are Google or Facebook that&#39;s peanuts, but if you are a startup you may not have that meny customers to experiment with. So, You should strive to minimize the number of samples (in this case customers) needed for eaxh experiment, which means that you must know the additional uncertainty you introduce when you make your sample sizes smaller. This will help you consider more carefully how the uncertainty propagates through the steps of your data analysis and how it affects your KPIs. . License You may use and modify this Jupyter notebook under the terms of the BSD license. .",
            "url": "https://lucacazzanti.github.io/abacus-canvas/statistics/estimation/gamma/2016/05/29/EstimatingGammaParameters.html",
            "relUrl": "/statistics/estimation/gamma/2016/05/29/EstimatingGammaParameters.html",
            "date": " • May 29, 2016"
        }
        
    
  
    
        ,"post3": {
            "title": "Know your questions!",
            "content": "Know your questions! . . Articulating precisely the question you are asking of the data gives clarity and focus to your data analysis. A complication is that often a data analysis flows through different stages in a non-linear way: you have some preliminary assumptions, you explore the data, you revise the assumptions, then notice a correlation between variables, go on a week-long deep-dive to investigate it until you realize it was nothing, and so on and so forth. Another complication is that, as you work through the stages of the analysis, you sometimes must revise or completely change the original, broader question based on what you are learning. Managing these challenges is essential to producing an impactful analysis that answers the right question and abides by the best practices. . Over the years I’ve found that being aware of the type of question I am asking at each stage of a data analysis helps me work through these challenges. I’ve also found that being deliberate about noticing the stage transitions in a data analysis project helps me avoid time-consuming detours and dead-ends. The key is to label each data analysis stage with the appropriate type of question. So, what are the types of questions? For a long time I did not have a crisp nomenclature to help me, so I followed my intuition and experience, with mixed success. Recently I came across the work of researchers Roger Peng, Elizabeth Matsui, and Jeff Leek who have come up with a crisp list of six types of questions relevant to data analysis: descriptive, exploratory, inferential, predictive, causal, and mechanistic. I now use their list to guide me through my data analyses. Below, I summarize the six types of questions and riff on them a bit with my own observations from my data analysis experiences. . The Six Types of Questions, according to Peng, Matsui, and Leek. . Peng, Matsui and Leek describe six types of questions for data analysis: . ###Descriptive In a descriptive data analysis, all you do is describe the data. Typical outputs of these analyses are simple aggregations, like counts, percentages, bar charts, probability density estimates, quantiles, maximum, minimum, and missing values. You do not attempt to explain or interpret these summary statistics, or relate them to other sources of information. Dashboards are an example output of a descriptive data analysis: they summarize the data and offer it up to human experts for interpretation. . Almost always, a descriptive analysis is my first step in a broader study/analysis. It gives me an idea of the data size, distribution, and types, and helps me form a first guess at the types of analyses achievable with the given the data set. In fact I would say that every data analysis should go through a descriptive stage, even if its scope is limited. If you jump straight to inference or prediction without describing the data, you may not be able to interpret your results, or, horrors, may draw the wrong conclusion. So do yourself a favor: describe your data first. There are even built-in tools in Python: pandas.DataFrame.describe() and Dato’s graphlab.SFrame.show() are good places to start, or fire up Tableau. There are no excuses! . Exploratory . Think of exploratory data analysis as the idea-generation phase of a study. During this phase you formulate hypotheses about the relationships between subsets of the data. Perhaps you notice that shoppers tend to buy more hot dogs and beer on warm weekends; perhaps you notice an increased presence of pleasure boats during the summer on a given waterway; or maybe you observe that some of the 1000s of time series in your data set could be grouped into a small number of clusters. Whatever the subject matter, I find the exploratory phase very fun because it gives me creative license to free-associate, cut, slice, transform, and visualize the data, and formulate and quickly de-risk –sometimes improbable– hypotheses. I experience a sense of excitement and of possibility that stimulates my creativity. . With all this excitement it’s easy to get carried away and mistake a generated hypothesis for a conclusion. Don’t do it! Instead, make a list of the hypotheses, and support each of them with charts and quick calculations. You can do this easily with the Jupyter notebook in Python or with Tableau. Then figure out which one to address further first, perhaps with the help of teammates or subject matter experts. This is a perfect opportunity to share your hypotheses with your data analysis team, which may include other data scientists, subject matter experts and business stakeholders. Careful though: make sure you manage the message if your audience is not familiar with data analysis workflows. You are still at the rough draft stage of your overall analysis and you don’t want your preliminary hypotheses to be misconstrued as conclusions by an eager, but not data-savvy, manager. . Inferential . Remember those hypotheses you generated during the exploratory phase? In an inferential analysis you seek to validate a hypothesis on a different data set, or infer that what you observed during the exploratory phase holds in general. Why would you want to do that? Well, think of polls and surveys. The available data are a subset collected from the overall population, but you are seeking to generalize what you learn from the sample to the entire population. Here’s a made-up example. Say you discover that in your data set of 1000 adults, those who exercise at least 30 minutes a day enjoy a 10% decrease in the risk of heart disease. An inferential analysis would tell you if this result holds for the entire adult population, and typically it would also give you a bound on the uncertainty around the result, which captures how robustly the result generalizes to the overall population. . Predictive . Which customers are likely to churn out of a subscription over the next 60 days? Which offer is more likely to entice them to renew their subscriptions? How many cargo vessels will enter the port of Seattle in June 2018? These are examples of predictive questions, which arise in many fields and often underpin entire business models: data-driven startups live or die depending on their ability to answer predictive questions. Sometimes it’s easy to confuse a predictive question with and inferential one. When I’m confused I remind myself that with inference I am testing how well a hypothesis generalizes to a larger, or distinct data set, while with prediction I am attempting to identify the factors that predict an outcome. Here’s another way to remember this: with inference, you are making statements about averages, or other aggregated statistics, over groups; with prediction, you are attempting to predict an individual outcome. . Causal . In a causal analysis, you seek to test a cause-effect relationship between variables. Causation is a stronger relationship than correlation, which merely identifies a link between the variables. Often business problems are answered well enough by a predictive analysis, but sometimes you may be interested in the causes of the relationship between variables, especially when you are attempting to interpret the results more deeply based on the subject matter (with the help of an expert). Just be mindful and do not confuse a successful predictive analysis, which builds on correlations between inputs and outputs, with a causal one. . Mechanistic . A mechanistic question deals with the exact mechanisms involved in a phenomenon. For example, a chemical reaction can be explained exactly and mechanistically in terms of molecular bonds breaking and forming. Other examples are post-hoc reconstruction of events and root-cause analyses. It’s difficult to answer mechanistic questions as part of a prediction or inference task without data from very-well controlled experiments. Frankly, I have never come across a situation that called for a mechanistic data analysis (except for debugging code …), although this reflects my personal experience and is in no way a general statement about data analysis. However, I did catch myself a few times attempting to explain in detail how a variable affected another one even though the analysis did not call for a mechanistic explanation. So, recognize mechanistic thinking when it comes up, and decide if it’s appropriate for the particular stage of the analysis. . Inference, Prediction, Training, and Testing . If you work with big data technologies, you may be able to describe, explore and predict directly on the entire population, without sampling. For example, in a customer churn prediction task, you may have access to the entire customer base, numbering in the millions. So, perhaps you may be tempted to not bother with estimating how well your prediction applies to data outside of the available data set. That would be a mistake. Even if you have access to the entire population at a particular time, the population will change: a customer base changes continuously, new customers arrive and old ones churn out. . Conversely, you may have access only to a very small sample of the overall data set, but need to ensure that your inference extends to the entire population. For inference and prediction then, you need to estimate how well your models generalize. In addition to specifying the sampling process, the standard way to de-risk the generalization is to set aside separate training and testing subsets from the available data. Design your model on the training set, and test it on the testing set. Even better, if your model depends on hyper parameters, you should also have a separate cross-validation data set. In fact, to gain confidence that your prediction will be robust, you should repeatedly and randomly select the training and testing sets and assess the performance on each random split. If the results are consistent across the splits, you are on the right path. . I mention the issue of splitting the data into separate training and testing sets in the context of the types of data analysis questions because it’s a pervasive issue no matter the type of question. It’s possible to be clear about the type of analysis and yet produce incorrect results, unless the data are properly separated in training and testing data. In fact, taking some time to design the training, testing, and cross validation procedures can help you clarify the type of data analysis question you are answering and help you catch mistakes early on. If you are a software developer, a loose comparison to this concept could be test-driven development: think about how you would test your conclusion first, then design your model. . Final remark . I typically find myself working on descriptive, exploratory, inferential, and predictive data analyses. The first two I consider mandatory components of broader data analysis projects, and doing a good job on them gives your project a solid foundation for the subsequent inference and prediction tasks. The latter two require careful consideration of the interplay between the available data, the way it was obtained, and the adopted models. If you are mindful of the type of question you are asking of the data at any one stage and think early about testing your models, your overall data analysis will be better positioned to be correct, relevant, and impactful. . Resources . What is the Question? by Jeff Leek and Roger Peng. | The Art of Data Science by Roger Peng and Elizabeth Matsui. | The Elements of the Data Analytic Style by Jeff Leek. |",
            "url": "https://lucacazzanti.github.io/abacus-canvas/data%20analysis/2016/04/07/data-analysis-questions.html",
            "relUrl": "/data%20analysis/2016/04/07/data-analysis-questions.html",
            "date": " • Apr 7, 2016"
        }
        
    
  
    
        ,"post4": {
            "title": "Merging Sensor Data Streams with Python Generators and Priority Queues",
            "content": "A recurring task in multi-sensor data processing is merging -- or interleaving -- data from multiple sensors while maintaining chronological order. For example, you may be combining temperature readings from different weather stations in a region, stringing together the locations of mobile devices recorded by different cell towers, or piecing together the routes taken by maritime vessels from the data they broadcast through the Automatic Identification System (AIS). . Often, the volume of data recorded by each sensor is large compared to the available RAM; other times each sensor&#39;s data fits in memory, but the merged, sorted stream does not; yet other times the available RAM is insufficient even for smaller sensor data volumes because the merge-and-sort operation is carried out on a small, lightweight compute node (Raspberry Pi anyone?). For these reasons, maintaining order with the naive approach of concatenating-then-sorting-in-memory is infeasible. Fortunately, we can use Python generators, iterators, and priority queues from the Python package heapq to achieve ordered, merged data streams from multiple sensors with low memory overhead. . Motivating use case: Vessel draft as a proxy for loading/unloading activity in ports . . Vessels periodically broadcast their draft -- the vertical distance between the waterline and the vessel&#39;s keel -- through the AIS protocol. Draft readings are used to ensure that loaded vessels to not run aground in shallower waters. Loading and unloading a vessel changes its draft, which can be used as an indicator of progress in port operations and as a proxy for maritime commerce. When multiple AIS data streams report the draft readings, we need to merge them into one unified stream to enable further vessel analytics, such as change detection. . Approach . In this post I assume that there are two AIS sensor networks and data from each are stored in two separte database tables. For simplicity I also assume there are only two vessels. AIS messages transmitted by the vessels are stored in the database tables in the order they arrive, along with a time stamp. In this post I use sqlite3 to simulate this system and use synthetic datasets to illustrate the key points. . To address the requirement to use little memory: use lazy evaluation and generators to iterate over the data streams to avoid creating intermediate data containers. In practice, this means iterating over database connection objects, not over the data in the database. | To address the requirement to keep sorted order: delegate sorting of the individual data streams to the database, then use the powerful heapq.merge() function to interleave the individual streams into one. | . These two techniques have been known to software developers for a long time, but they mey be new to scientists (like me) who do not have formal training in software engineering or databases, but need to analyze large data sets. . Packages and helper functions . import heapq import sqlite3 import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) %matplotlib inline def head_db(table_name, n): &quot;&quot;&quot; Print the first n records from table_name &quot;&quot;&quot; query = &quot;SELECT * FROM %s LIMIT %s&quot; % (table_name, n) conn = sqlite3.connect(&#39;vessel_draft.sqlite&#39;) conn.row_factory = sqlite3.Row result = conn.execute(query) for row in result: res = {key: row[key] for key in row.keys()} print res conn.close() def plot_timeseries(data, **kwargs): &quot;&quot;&quot; Plot the time series of draft readings for one vessel. &quot;&quot;&quot; (timestamp, vessel_id, draft) = zip(*data) plt.step(timestamp, draft, **kwargs) plt.xlabel(&#39;Timestamp&#39;) plt.ylabel(&#39;Draft&#39;) plt.title(&#39;Vessel ID = &#39; + str(vessel_id[0])) plt.ylim([min(draft) - 1, max(draft)+1]) plt.xlim([min(timestamp)-1, max(timestamp)+2]) def plot_timeseries_from_tables(table_names): &quot;&quot;&quot; Plot the time series of draft readings for one vessel from 2 sensors. &quot;&quot;&quot; plt.figure() conn = sqlite3.connect(&#39;vessel_draft.sqlite&#39;) query = &quot;SELECT * FROM %s&quot; % (table_names[0], ) result = conn.execute(query) plot_timeseries(result, where=&#39;post&#39;, marker=&#39;o&#39;, color=&#39;r&#39;, alpha=0.5) plt.hold(True) query = &quot;SELECT * FROM %s &quot; % (table_names[1], ) result = conn.execute(query) plot_timeseries(result, where=&#39;post&#39;, marker=&#39;o&#39;, color=&#39;b&#39;, alpha=0.5) plt.legend([&#39;Sensor 1&#39;, &#39;Sensor 2&#39;], loc=&#39;lower right&#39;) conn.close() def plot_multi_series(data): &quot;&quot;&quot; Plot the time series of draft readings for 2 vessel from 2 sensors. &quot;&quot;&quot; vessel1 = filter(lambda row: row[0] == 1, data) vessel2 = filter(lambda row: row[0] == 2, data) plt.figure(figsize=(10,6)) plt.subplot(121) plot_timeseries(map(lambda row: (row[1], row[0], row[2]), vessel1), where=&#39;post&#39;, marker=&#39;o&#39;, color=&#39;darkviolet&#39;) plt.subplot(122) plot_timeseries(map(lambda row: (row[1], row[0], row[2]), vessel2), where=&#39;post&#39;, marker=&#39;o&#39;, color=&#39;darkviolet&#39;) . Basic scenario: Two sensors and one vessel . Assume there are two sensors (two database files), each one recording the draft of a single vessel, the timestamp of the draft reading and the vessel id. When a new reading is available, each sensor appends the value and the timestamp to its assigned database table. Below is an example of what the data sources might look like. Note that the timestamps recorded by one sensor need not match the timestamps of the other, because the sensors operate asynchronosuly and each one records only the data it observes. This can be seen in the plot: some draft readings are common to the two sensors (violet color), but each sensor has access to readings not available to the other (red and blue color) . head_db(&#39;sensor1_single_vessel&#39;, 7) . {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 1} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 2} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 3} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 6} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 7} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 8} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 11} . head_db(&#39;sensor2_single_vessel&#39;, 7) . {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 1} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 3} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 4} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 6} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 7} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 8} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 9} . plot_timeseries_from_tables([&#39;sensor1_single_vessel&#39;, &#39;sensor2_single_vessel&#39;]) . Let&#39;s join the draft readings from the two sensors into one time series ... . table_names = [&#39;sensor1_single_vessel&#39;, &#39;sensor2_single_vessel&#39;] queries = [&quot;SELECT * FROM %s&quot; % table_name for table_name in table_names] # Open one DB connection per table (sensor) conns = [sqlite3.connect(&#39;vessel_draft.sqlite&#39;) for i in range(len(table_names))] params = zip(conns, queries) readers = [conn.execute(query) for (conn, query) in params] merged = heapq.merge(*readers) plot_timeseries(merged, where=&#39;post&#39;, color=&#39;darkviolet&#39;, marker=&#39;o&#39;) for conn in conns: conn.close() . Voila&#39; a merged series of sensor readings, sorted by time! . Why does this work and why is this efficient? We rely on lazy evaluation: the readers list contains generators over the query results, and the merged result is itself a generator. heapq.merge() does not read the data in memory to sort it. Instead it creates only the internal data structures necessary to index the underlying data, and returns a generator. When one iterates over this generator, the order in which the items are yielded is the desired sorted order over the merged streams. . Let&#39;s take a look at the code in more detail. . Lay out the parameters we will use and open one connection per table (sensor) to the database. We keep the connection objects in the list conns. | . table_names = [&#39;sensor1_single_vessel&#39;, &#39;sensor2_single_vessel&#39;] queries = [&quot;SELECT * FROM %s&quot; % table_name for table_name in table_names] # Open one DB connection per table (sensor) conns = [sqlite3.connect(&#39;vessel_draft.sqlite&#39;) for i in range(len(table_names))] . Set up the queries. The readers list contains iterators over the query results, but the queries are not yet executed. Later, iterating over each reader object will yield a sqlite3.Row object from teh corresponding database table.params = zip(conns, queries) readers = [conn.execute(query) for (conn, query) in params] . | Finally, merge the iterators over the data:merged = heapq.merge(*readers) . Note that merged is itself a generator, thus very little memory is used until it is consumed by the plot function. It is really this simple! | . More realistic scenario - Two sensors and multiple vessels . But wait, there are many vessels, not just one: A vessel draft data stream is more like the one below, where readings for different vessels may arrive at the same time and different vessel ids are interleaved: . head_db(&#39;sensor1_multi_vessel&#39;, 6) . {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 1} {&#39;draft&#39;: 5.0, &#39;id&#39;: 2, &#39;time&#39;: 1} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 2} {&#39;draft&#39;: 7.0, &#39;id&#39;: 2, &#39;time&#39;: 2} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 3} {&#39;draft&#39;: 7.0, &#39;id&#39;: 2, &#39;time&#39;: 3} . This presents a complication: we want to maintain the chronological order of the sensor readings for each vessel, but the data are ordered globally by timestamp because each reading is recorded in the order it arrives, irrespective of the vessel. Feeding sensor tuples as they are to heapq.merge() will produce an undesired result. To address this problem we do the following: . Group together the readings from the same vessels: we can do this simply by modifying the SQL query and use the ORDER BY contruct. This achieves vertical ordering of the readings from each sensor. | Re-order the tuple elements presented to heapq.merge() in the same way as the ORDER BY construct. We do this by declaring explicitly the order of the columns in the SQL SELECT statment. This ensures the correct horizontal ordering for the nested sorted index underlying heapq.merge(). | . table_names = [&#39;sensor1_multi_vessel&#39;, &#39;sensor2_multi_vessel&#39;] # MODIFIED QUERY: nested order by id, time, draft and uses ORDER BY queries = [&quot;SELECT id, time, draft FROM %s ORDER BY id, time&quot; % table_name for table_name in table_names] # Open one DB connection per table (sensor) conns = [sqlite3.connect(&#39;vessel_draft.sqlite&#39;) for i in range(len(table_names))] params = zip(conns, queries) readers = [conn.execute(query) for (conn, query) in params] merged = heapq.merge(*readers) # Store in list for display result = list(merged) for conn in conns: conn.close() . print &quot;(id, time, draft)&quot; for row in result[:10]: print row print &quot;...&quot; for row in result[26:36]: print row plot_multi_series(result) . (id, time, draft) (1, 1, 12.0) (1, 1, 12.0) (1, 2, 12.0) (1, 3, 12.0) (1, 3, 12.0) (1, 4, 12.0) (1, 6, 12.0) (1, 6, 12.0) (1, 7, 18.0) (1, 7, 18.0) ... (2, 1, 5.0) (2, 2, 7.0) (2, 2, 7.0) (2, 3, 7.0) (2, 3, 7.0) (2, 4, 7.0) (2, 4, 7.0) (2, 5, 7.0) (2, 6, 10.0) (2, 7, 10.0) . There it is, a merged stream where the sensor readings for each vessel are ordered by time, and the corresponding plots. . Detecting changes in streaming sensor data . Tipically we are interested in changes in vessel draft, not in the entire sequence of readings, because draft changes slowly compared to the sampling period of the sensor and because changes in values and the time at which they occur carry the key information about port activities and vessel behavior. Furthermore, different sensors may report redundant readings (same value, same timestamp, same vessel id) that we should filter out. We can easily filter out the redundant information and the readings for which the draft has not changed with the powerful unique_justseen recipe, which filters an iterable, keeping an element only if it differs from the previous one. In keeping with our goal of lazy computing, it returns an iterator over the filtered sequence, not the actual data. . I downloaded all the recipes available on the itertools doc page and put them in a module called itertools_recipes. I use the recipes often, as below: . from itertools_recipes import unique_justseen table_names = [&#39;sensor1_multi_vessel&#39;, &#39;sensor2_multi_vessel&#39;] queries = [&quot;SELECT id, time, draft FROM %s ORDER BY id, time&quot; % table_name for table_name in table_names] # Open one DB connection per table (sensor) conns = [sqlite3.connect(&#39;vessel_draft.sqlite&#39;) for i in range(len(table_names))] params = zip(conns, queries) readers = [conn.execute(query) for (conn, query) in params] merged = heapq.merge(*readers) # THIS IS IT -&gt; Change detection change_points = unique_justseen(merged, key=lambda r: (r[0], r[2])) change_pts_result = list(change_points) for conn in conns: conn.close() print &quot;(id, time, draft)&quot; for row in change_pts_result: print row plot_multi_series(change_pts_result) . (id, time, draft) (1, 1, 12.0) (1, 7, 18.0) (1, 15, 15.0) (2, 1, 5.0) (2, 2, 7.0) (2, 6, 10.0) (2, 16, 7.0) . And here they are, merged, parsimonious time series that capture when the draft changes, for two vessel, from two different sensors. . Note that I specified a key for unique_everseen to understand which part of the tuples to use for the queality comparison: I want to determine uniqueness only by the vessel id and draft values (tuple elements 0 and 2). Since the tuples are ordered by time, this results in sorted stream with duplicated draft readings for each vessel filtered out. . Final remarks: scaling, distributing, batch and real-time processing . Don&#39;t let the simplicity of this approach fool you. It is a powerful way to combine many sources of data on small-ish compute nodes and a fundamental building block for more sophisticated analyses. In my work I use versions of it to process $ approx$ 500 million AIS messages from 2-5 sources of AIS data from a SQL Server every month. On my HortonWorks VM running on a quad-core Intel processor, this takes approximately 15 minutes and a small blip in memory usage. It is a perfect match for batch processing of slowly-changing attributes in large datasets and it has been a huge helper for me when I need to prepare data for machine learning algorithms and visualizations. . I deal mostly with historical analyses of data, for which higher latency is acceptable and the entire data over the period of analysis is available. Therefore, delegating the initial sorting and row ordering of the individual streams to the database is feasible and indeed desirable. This approach can be extended to geographically distributed data sources, for example to a scenario where data from hundreds of storage-capable AIS receivers provide the data streams for periodic merging. . I used the term stream to indicate that the data are read in sequence; it is not to be confused with real-time. If you need to merge and sort in real time, then you may need to read small buffers from the data sources in memory and perform the sorting and merging on each buffer within the Python program, accepting that you may obtain only approximate results on time windows larger than the buffer size: speed costs accuracy. Of course, then other frameworks, like Storm and SparkStreaming may be more appropriate, and you may also consider Lambda Architecture in your system design. . Further reading . Sorting 1M 32-bit integers in 2 MB of RAM using Python, by Guido van Rossum | Let your mind be blown by Dave Beazley&#39;s PyCon presentation on Python generators. | Background on AIS | Binary heap on Wikipedia, the data structure that enables the magic in heapq.merge() | . License You may use and modify this Jupyter notebook under the terms of the BSD license. Download the the supporting sqlite database. .",
            "url": "https://lucacazzanti.github.io/abacus-canvas/sensors/streaming%20data/python%20generators/2016/03/19/SensorDataStreamProcessing.html",
            "relUrl": "/sensors/streaming%20data/python%20generators/2016/03/19/SensorDataStreamProcessing.html",
            "date": " • Mar 19, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Luca Cazzanti, Ph.D.",
          "content": "I am a scientist and a technologist with a successful track record in both academic research and product commercialization. I come from a statistical signal processing and data analysis background, which I developed into machine learning and data science expertise and applied to diverse fields. I have worked in both large, structured organizations and small, fluid startups, including my own AI consulting and advising outfit. I lead technical teams and communicate with the business units and subject matter experts at the appropriate level of granularity. At the same time, I remain solidly technical and contribute product specifications, system designs, visualizations, exploratory data analyses, prototypes, insights. I most enjoy connecting concepts from machine learning, AI and tools from data science to the semantics, constraints, and requirements of specific problems to deliver solutions that are practical, impactful, and backed by best practices. Visit www.lucacazzanti.net for more information on my past projects, a list of scientific publications, and my CV. .",
          "url": "https://lucacazzanti.github.io/abacus-canvas/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lucacazzanti.github.io/abacus-canvas/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}