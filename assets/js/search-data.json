{
  
    
        "post0": {
            "title": "Attacking corner kicks - A case study",
            "content": "What is this note about and who is it for? . I briefly present the structure of a typical analyst report about an opponent’s attacking corner kicks in soccer. Then, I walk through an example of this type of report, focusing on Spanish club SD Eibar. . This note will interest you if: . You are new to soccer match analysis and curious about what a report on attacking corner kicks looks like. | You are a match analyst and are interested in similar work. | You like the beautiful game. | . This note is an expanded version of an assignment I completed as part of FC Barcelona’s Innovation Hub course on Set Piece Analysis, which is a requirement for their Certificate in Advanced Football Tactical Analysis. In the last section you will find additional information, including the orginal powerpoint presentation, supporting videos, and tools used. . Content and Structure of a Corner Kick Analysis Report . In elite soccer, match analysts support the coaching staff with analyses of their own team and the opponents. One of their duties is to prepare reports focusing on the opponent’s set pieces, of which corner kicks are one component. Corner kick analyses can be further divived in defensive (when the opponent is defending against a rival’s corner kick) and attacking (when the opponent is taking the corner kick). The content and structure of these two views is similar, but here I focus only on the attacking corner kicks. . The content of the report will strongly depend on the specific interests of the coaching staff. The format and structure must also adapt to their needs: some coaches prefer a short video and verbal talking points; others prefer a PowerPoint presentation with embedded videos; some coaches are more quantitative than others, which affects the balance of numbers and images in the report. Finally, there should be absolute clarity on the timeline for delivering the reports to the coaching staff. It is the job of the analyst to align the content, format, and deadline of the report to the coaches’ needs. . Notwithstanding the need to adapt the report to the coaches, there are recurring, fundamental aspects of attacking corner kicks that will be of interest to just about any coach in any club. Your report must identify how the particular opponent interprets these fundamentals. . Main characteristics to observe in attacking corner kicks . Structure: How is the team organized just before the kick? In parallel lines inside/outside the small area? In a “conga line” down the middle? Clustered at the near/far post? | How many players in the small area / penalty area? | How many at the top of the area / preventive coverage? | What are the player locations on short kicks? | . | Movements: How, where, and when do the players move during the kick? Movement before the kick? Or only immediately before? | Any blocks, decoy runs, attempts to drag defenders to create space? | . | Trajectory: How is the ball kicked? Inswinging | Outswinging | Flat | Short/indirect | . | Delivery location: Where does the ball arrive on the pitch? Near post | Far post | Middle of small area | Penalty kick spot | Are attempts direct or do they arise from second balls/indirect flicks? | . | Habitual kickers: Which players usually take corner kicks? Are they right/left footed? | Consequences of right/left footedness and side of the corner kick on the inswinging/outswinging trajectory | . | Habitual receivers: Which players usually receive the ball and/or attempt a shot? What are their main characteristics? | Do defenders push up to attempt to score? | . | Performance: Summary counts and statistics on the team’s success in creating chances and scoring goals from corner kicks. | Patterns and correlations: Point out any correlations between the above characteristics. Example: inswinging kicks from the right of the goal to the near post yield more shots on goal. | . Finally, keep it short! Coaches have limited time, and need to digest various reports - not just corner kicks - so focus on the main points and be brief. . . Trajectory: direct kicks , short pass not used as much . Shooters: only right-footed; #14 -Orellana and #21 Leon . Structure: 5 players in area; 6 for unfavorable score . Clearing and defense: 2-3 players at top of area; 1-2 defense . PLAYS . Inswinging: ball to near post for direct attempt or touch to second player in small area. Overloading near post to create attempt or create space for players in small area. . Outswinging: direct ball to far post/penalty zone. Fake movement to near post to create space at far post. . In general: players align immediately outside the small area in the penalty zone and rush in for attempt or to drag opponents to create space. . PERFORMANCE . Inswinging/near post → more attempts and shots . Outswinging/far post → more frequent . . Habitual structure for . outswinging corners . 1-4 line structure at start of play , both inswinging and outswinging , but with some differences . . Habitual starting structures . . . 0 or 1 players in the small area . 5 players in the area . 6 when unfavorable score . . 1-4 line structure at start of corner play . 2 or 3 players outside the area at the top-center . . Habitual Trajectory and Delivery Location . . . Inswinging . near post or small area . Outswinging . Penalty spot , in front of small area . Short used less frequently to surprise opponent . Habitual kickers . __#14 - Fabian Orellana (winger) __ . #21 - Pedro Leon (winger) . Both left and right side . There are __ __no left-footed kickers . . → __ __right __ side corner is __ inswinging __ → __ near post . → __ __left __ side corner is __ outswinging __ → __ penalty/far post . . Habitual Functions . Other players in the area . Receive , drag , block . # 6 - Sergio A. . # 8 - P. Diop . # 12 - P. Oliveira . Players at the top of the area . Recover ball losses , shoot from outside . # 22 - Inui . __# 16 - de Blasis __ . # 10 - Exposito . Habitual receivers . # 9 - Sergi Enrich , center-forward , 1.83m , good with headers , strong , good at maintaining position . #17 - Kike Garcia , center-forward , 1 ,86 m , good with headers . # 5 - Xavi Etxeita , center-back , 1.86 m , good with headers . . . . Images and info: Transfermarkt.com . . Performance . MOST OF THE ATTEMPTS COME FROM __NEAR POST OR PENALTY ZONE __ DELIVERY . __INSWINGING __ PRODUCES __MORE ATTEMPTS __ THAN OUTSWINGING . . . SUMMARY . Inswinging/near post/penalty zone → more attempts and goals (more effective) . Outswinging/far post → more frequent but fewer attempts (less effective) . . Inswinging . Overload the near post to obtain an attempt or to attract the opponent and create space in the small area in front of the goal or far post. . . . Outswinging . Runs to the near and far posts to drag/attracts opponents and create space in the penalty zone for strikers to head the ball from that zone. . TESTING 1 . TESTING 2 . TESTING 3 .",
            "url": "https://lucacazzanti.github.io/abacus-canvas/set%20piece%20analysis/corner%20kicks/opponent%20report/pre-match%20report/soccer%20analytics/dartfish/2022/06/01/corner-kicks.html",
            "relUrl": "/set%20piece%20analysis/corner%20kicks/opponent%20report/pre-match%20report/soccer%20analytics/dartfish/2022/06/01/corner-kicks.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Effectiveness and location of pressure actions in soccer",
            "content": "Summary . In this activity I explore the relationship between team performance and team pressure behavior using data from the English Premiere League season 2020-2021. I focus on Manchester City and Sheffield United, respectively the winner and bottom finisher for that season. I describe how points, goals-against (GA) and shots-on-target-against (SoTA) are related to the success rate and the field location of teams&#39; pressure acvitivies, and how these relationships explai the different final placements achieved by high-perfoming and low-performing teams. The analyzed metrics are related to passes-per-defensive action (PPDA), a summary metric of a team&#39;s efficiency in applying defensive pressure, but in this note I do not look at this metric. Instead I focus on readily available, first-principles metrics that are nonetheless informative with only a minimum of data processing. While my observations are qualitative, they are supported by quantitative and exploratory analyses performed in Python. . While high-press (applying defensive pressure in the opponent&#39;s third of the pitch) is not required for teams to win games, there exist a correlation between winning (points achieved) and using high-press. However, it is not sufficient to simply apply pressure (proximity within 5 meters of an opponent with the ball) in quantity in the attacking third: the quality of the pressure is also important, as measured by the success rate of pressure events (success is winning possession). Furthermore, it is important to also look at the normalized profiles of pressure events across the thirds of the pitch to characterize how a team chooses to distribute its totall pressure effort throughout a match in the three zones: raw counts, even segmented by pitch thirds, are misleading. . For the purposes of analysing a team&#39;s pressure effectiveness, metrics closer to the played game such as GA and SoTA are more relevant than points earned. For these metrics there is a stronger correlation with the success rate of pressure and the proportion of pressure applied in the attacking third. . The statistics of Sheffield United (bottom-finisher) and Manchester City (EPL winner), are surprising at first if we only consider points won. Sheffield produced the highest number of pressures, both in total and in the attacking third, but finished last. Manchester City, conversely, had the fewest number of pressures, yet was the winner. The distinguishing characteristic for Man City is the highest success rate of the pressures and its stronger propensity toward the high-press. . Success in pressure depends on many factors. It is not limited to the individual pressure event that leads to winning possession, but it also depends on the teammates covering the spaces or marking 1:1 correctly, applying their chosen pressure structures correctly, being ready on their jumps-to-pressure, and on any dynamic advantage of the individual players. Manchester City possesses more of these team and individual characteristics than Sheffield United and it is likely better able to choose the moments for applying pressure, increasing the efficiency of pressure activities. This, together with the fact that they practice a higher press, contributes to their low GA and SoTA metrics. . Data . The data was downloaded from FBRef. This exercise is part of the requirements for the course &quot;High-press in Football&quot; from Barca Innovation Hub . Set up the Python environment, load and prepare data . import pandas as pd from google.colab import data_table import seaborn as sns sns.set_theme(&#39;talk&#39;) import matplotlib.pyplot as plt . . # Read the Squads and Players --&gt; defensive actions statistics data_source = &#39;https://fbref.com/en/comps/9/10728/defense/2020-2021-Premier-League-Stats#stats_squads_defense_for&#39; pressure_data = pd.read_html(data_source)[0] . . #Keep only what we need pressure_data = pressure_data[[&#39;Unnamed: 0_level_0&#39;, &#39;Pressures&#39;]].copy() pressure_data.columns = pressure_data.columns.droplevel() . . # Calculate the percentage of pressure events by third of the field --&gt; will use it later # Rounding the numbers for readability pressure_data[&#39;Def 3rd %&#39;] = round(pressure_data[&#39;Def 3rd&#39;]/pressure_data[&#39;Press&#39;] * 100,1) pressure_data[&#39;Mid 3rd %&#39;] = round(pressure_data[&#39;Mid 3rd&#39;]/pressure_data[&#39;Press&#39;] * 100, 1) pressure_data[&#39;Att 3rd %&#39;] = round(pressure_data[&#39;Att 3rd&#39;]/pressure_data[&#39;Press&#39;] * 100, 1) . . # Read the Squads and Players --&gt; goalkeepeing statistics goalkeeping_source=&#39;https://fbref.com/en/comps/9/10728/keepers/2020-2021-Premier-League-Stats#stats_squads_keeper_for&#39; goalkeeping_data = pd.read_html(goalkeeping_source)[0] . goalkeeping_data.drop([&#39;Penalty Kicks&#39;, &#39;Playing Time&#39;, &#39;Unnamed: 1_level_0&#39;], axis=1, inplace=True, level=0) goalkeeping_data.columns = goalkeeping_data.columns.droplevel() . # Calculate the total points for the season for each team --&gt; will use it later goalkeeping_data[&#39;Points&#39;] = 3*goalkeeping_data[&#39;W&#39;] + goalkeeping_data[&#39;D&#39;] # Keep only what we need goalkeeping_data.drop([&#39;Saves&#39;, &#39;Save%&#39;, &#39;W&#39;, &#39;D&#39;, &#39;L&#39;, &#39;CS&#39;, &#39;CS%&#39;], axis=1, inplace=True) . . all_data = pd.merge(pressure_data, goalkeeping_data, on=&#39;Squad&#39;) data_table.DataTable(all_data, include_index=False) . Squad Press Succ % Def 3rd Mid 3rd Att 3rd Def 3rd % Mid 3rd % Att 3rd % GA GA90 SoTA Points . 0 Arsenal | 4685 | 1331 | 28.4 | 1499 | 1897 | 1289 | 32.0 | 40.5 | 27.5 | 39 | 1.03 | 128 | 61 | . 1 Aston Villa | 5446 | 1473 | 27.0 | 1888 | 2277 | 1281 | 34.7 | 41.8 | 23.5 | 46 | 1.21 | 177 | 55 | . 2 Brighton | 5179 | 1638 | 31.6 | 1732 | 2164 | 1283 | 33.4 | 41.8 | 24.8 | 46 | 1.21 | 117 | 41 | . 3 Burnley | 4654 | 1282 | 27.5 | 1381 | 2041 | 1232 | 29.7 | 43.9 | 26.5 | 55 | 1.45 | 179 | 39 | . 4 Chelsea | 5376 | 1668 | 31.0 | 1692 | 2388 | 1296 | 31.5 | 44.4 | 24.1 | 36 | 0.95 | 103 | 67 | . 5 Crystal Palace | 5700 | 1502 | 26.4 | 2314 | 2378 | 1008 | 40.6 | 41.7 | 17.7 | 66 | 1.74 | 173 | 44 | . 6 Everton | 5660 | 1604 | 28.3 | 2220 | 2364 | 1076 | 39.2 | 41.8 | 19.0 | 48 | 1.26 | 158 | 59 | . 7 Fulham | 5145 | 1537 | 29.9 | 1720 | 2157 | 1268 | 33.4 | 41.9 | 24.6 | 53 | 1.39 | 164 | 28 | . 8 Leeds United | 6661 | 1972 | 29.6 | 2341 | 2885 | 1435 | 35.1 | 43.3 | 21.5 | 54 | 1.42 | 189 | 59 | . 9 Leicester City | 5142 | 1629 | 31.7 | 1797 | 2214 | 1131 | 34.9 | 43.1 | 22.0 | 50 | 1.32 | 134 | 66 | . 10 Liverpool | 5394 | 1707 | 31.6 | 1339 | 2329 | 1726 | 24.8 | 43.2 | 32.0 | 42 | 1.11 | 137 | 69 | . 11 Manchester City | 4560 | 1462 | 32.1 | 1167 | 2030 | 1363 | 25.6 | 44.5 | 29.9 | 32 | 0.84 | 89 | 86 | . 12 Manchester Utd | 5041 | 1490 | 29.6 | 1537 | 2164 | 1340 | 30.5 | 42.9 | 26.6 | 44 | 1.16 | 135 | 74 | . 13 Newcastle Utd | 5423 | 1346 | 24.8 | 2148 | 2247 | 1028 | 39.6 | 41.4 | 19.0 | 62 | 1.63 | 179 | 45 | . 14 Sheffield Utd | 6123 | 1518 | 24.8 | 2080 | 2631 | 1412 | 34.0 | 43.0 | 23.1 | 63 | 1.66 | 205 | 23 | . 15 Southampton | 5714 | 1783 | 31.2 | 1970 | 2500 | 1244 | 34.5 | 43.8 | 21.8 | 68 | 1.79 | 169 | 43 | . 16 Tottenham | 5871 | 1643 | 28.0 | 2124 | 2563 | 1184 | 36.2 | 43.7 | 20.2 | 45 | 1.18 | 144 | 62 | . 17 West Brom | 5491 | 1531 | 27.9 | 2119 | 2268 | 1104 | 38.6 | 41.3 | 20.1 | 76 | 2.00 | 235 | 26 | . 18 West Ham | 4989 | 1342 | 26.9 | 1882 | 2133 | 974 | 37.7 | 42.8 | 19.5 | 47 | 1.24 | 143 | 65 | . 19 Wolves | 5195 | 1558 | 30.0 | 2105 | 2187 | 903 | 40.5 | 42.1 | 17.4 | 52 | 1.37 | 144 | 45 | . # Define some functions to help the visualizations def plot_squads(df, x, y, figsize=(10,10), fontdict=dict(size=10), title=&#39;&#39;, xlabel=&#39;&#39;, ylabel=&#39;&#39;, x_offset=0.1, y_offset=0.1): plt.figure(figsize=figsize) sns.scatterplot(data=df, x=x, y=y) plt.title(title) plt.ylabel(ylabel) plt.xlabel(xlabel) for _, row in df.iterrows(): plt.text(row[x]+x_offset, row[y]+y_offset, row[&#39;Squad&#39;], fontdict=fontdict) def subplot_squads(df, x, y, ax, fontdict=dict(size=10), xlabel=&#39;&#39;, ylabel=&#39;&#39;, x_offset=0.1, y_offset=0.1): sns.regplot(data=df, x=x, y=y, ax=ax) ax.set_ylabel(ylabel) ax.set_xlabel(xlabel) for _, row in df.iterrows(): ax.text(row[x]+x_offset, row[y]+y_offset, row[&#39;Squad&#39;], fontdict=fontdict) . . . Comparing the total number and success rate of pressure events . The tables below show defensive pressure statistics for teams in the 2020-21 English Premire League, sorted in two different ways: by total number of pressure events and by the team&#39;s success rate of the pressure events. . It is immediately obvious that the total number and the success rate do not follow the same, or even similar, order. In fact, in some cases the order is completely reversed: Manchester City had the lowest number of pressure events, but the highest success rate. Conversely, Sheffield United had the second highest number of pressures, yet the lowest success rate. . Order by Total Pressure Events Order by Percent of Successful Pressure Events . | | . We can gain deeper insight by plotting the data and computing summary statistics, as below. From the scatterplot, we can make the following qualitative statements: . There is a weak relationship between the number of pressure events and the percentage of successful pressures for each team. This could be because each team has an intrinsic efficiency at pressure events which is independent of the number of events they produce, and instead depdends on their ability and style of play. In brief: just because a team produces more pressure events, it does not mean that it also produces successful ones. | There could be a slightly inverse relationship between pressure events and success rate. A possible reason is that teams with a high number of events could be playing a more defensive style where the pressure is applied closer to their own half of the field. In these cases pressure events prioritize containment of the opponent over winning possession. Furthermore, teams that play more defensively tend to cede ball possession to the opponent. These two factors increase the opportunities of applying pressure but the intrinsic efficiency of the team remains the same, thus overall the success rate is slightly lower. | At the low end of the number of pressures: Manchester City, Arsenal, and Burnley produced very similar pressure, but Manchester City stands out as having 3.5% higher success | At the low end of success rate: Newcastle United and Sheffield United have very similar succeess rates, but Sheffield produced 700 more pressures, while Necastle;s pressure events are near the overall average of 5372 | Leeds United stands out as an outlier for number of pressures, 6661, while their success rate of 29.6% is very close the the avearge of 28.9%. | . title = &#39;Percent of successful pressure events vs. total pressure events&#39; ylabel = &#39;Percent of successful pressure events&#39; xlabel = &#39;Total number of pressure events&#39; plot_squads(all_data,x=&#39;Press&#39;, y=&#39;%&#39;,title=title, xlabel=xlabel, ylabel=ylabel) . all_data.describe() . Press Succ % Def 3rd Mid 3rd Att 3rd Def 3rd % Mid 3rd % Att 3rd % GA GA90 SoTA Points . count 20.00000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | 20.000000 | . mean 5372.45000 | 1550.800000 | 28.915000 | 1852.750000 | 2290.850000 | 1228.850000 | 34.325000 | 42.645000 | 23.040000 | 51.200000 | 1.348000 | 155.100000 | 52.850000 | . std 506.83969 | 165.806355 | 2.264201 | 339.620357 | 228.299497 | 189.164444 | 4.480704 | 1.113305 | 4.012795 | 11.251199 | 0.296019 | 34.857454 | 16.887476 | . min 4560.00000 | 1282.000000 | 24.800000 | 1167.000000 | 1897.000000 | 903.000000 | 24.800000 | 40.500000 | 17.400000 | 32.000000 | 0.840000 | 89.000000 | 23.000000 | . 25% 5116.75000 | 1470.250000 | 27.375000 | 1653.250000 | 2162.250000 | 1097.000000 | 31.875000 | 41.800000 | 19.950000 | 44.750000 | 1.175000 | 134.750000 | 42.500000 | . 50% 5385.00000 | 1534.000000 | 29.000000 | 1885.000000 | 2257.500000 | 1256.000000 | 34.600000 | 42.850000 | 22.550000 | 49.000000 | 1.290000 | 151.000000 | 57.000000 | . 75% 5670.00000 | 1639.250000 | 31.050000 | 2120.250000 | 2380.500000 | 1307.000000 | 37.925000 | 43.400000 | 25.225000 | 56.750000 | 1.495000 | 177.500000 | 65.250000 | . max 6661.00000 | 1972.000000 | 32.100000 | 2341.000000 | 2885.000000 | 1726.000000 | 40.600000 | 44.500000 | 32.000000 | 76.000000 | 2.000000 | 235.000000 | 86.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; fig, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(20,10)) subplot_squads(all_data, x=&#39;%&#39;, y=&#39;Points&#39;, ylabel=&#39;Points&#39;, xlabel=&#39;Percent of successful pressure events&#39;, ax=ax1) subplot_squads(all_data, x=&#39;Press&#39;, y=&#39;Points&#39;, xlabel=&#39;Total number of pressure events&#39;, ax=ax2) . points_press_corr = all_data[&#39;Points&#39;].corr(all_data[&#39;Press&#39;]) points_success_corr = all_data[&#39;Points&#39;].corr(all_data[&#39;%&#39;]) print(&#39;Correlation between points and number of pressures: {:f}&#39;.format(points_press_corr)) print(&#39;Correlation between points and success rate of pressures: {:f}&#39;.format(points_success_corr)) . Correlation between points and number of pressures: -0.279062 Correlation between points and success rate of pressures: 0.448378 . . Pressure events by field location . To understand better the pressure characteristics of the teams, we can look at where the teams apply pressure: in their defensive third, middle third, or attacking third (high-press) of the pitch. The table below shows the teams ordered by the number of pressure events in the attacking third. We see that: . Liverpool produce the most pressures in the attacking third, a difference in rank of +9 compared to their total pressure events. | Leeds United and Sheffield United are 2nd and 3rd in most pressures in the attacking third, which is consistent with the fact that these two teams produce the most overall pressure events. | Manchester City are in 4th place, a difference in rank of +16 compared to their total pressure events. | . . We see that Sheffield and Leeds were among the leaders for pressure events applied in the attacking third, but their final placement in the table is quite different, with Sheffield finishing last, and Leeds in 9th place. Manchester City won the championsip, but had less pressure events overall and in the attacking third than these two teams. The explanation is that we need to look at the distribution of a team&#39;s pressure events in the three parts of the pitch, not just at the raw total counts. This gives us a picture of a team&#39;s preferred zone of pressure application throughout the match. Thus we look at the normalized profile of the pressure events for each team, not just the raw counts. The following figure shows that there is a stonger correlation between points won and the percentage of pressure in attacking third (0.38), compared to considering the just raw numbers (0.23). . Based on these considerations, and the ones from the previous section, we understand that a team&#39;s performance during the season depends on the relative location of their pressure activity and on the quality (success rate) of their efforts. . fig, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(20,10)) subplot_squads(all_data, x=&#39;Att 3rd&#39;, y=&#39;Points&#39;, ylabel=&#39;Points&#39;, xlabel=&#39;Number of pressures in attacking third&#39;, ax=ax1) subplot_squads(all_data, x=&#39;Att 3rd %&#39;, y=&#39;Points&#39;, xlabel=&#39;Percent of pressures in attacking third&#39;, ax=ax2) . points_att_pct_corr = all_data[&#39;Points&#39;].corr(all_data[&#39;Att 3rd %&#39;]) points_att_num_corr = all_data[&#39;Points&#39;].corr(all_data[&#39;Att 3rd&#39;]) print(&#39;Correlation between points and number of pressures in the attacking third: {:f}&#39;.format(points_att_num_corr)) print(&#39;Correlation between points and percent of pressure in the attacking third: {:f}&#39;.format(points_att_pct_corr)) . Correlation between points and number of pressures in the attacking third: 0.231986 Correlation between points and percent of pressure in the attacking third: 0.378874 . . Relationship to goals-against (GA) and shots-on-target-against (SoTA) . In the previous sections we looked at the relationship between a team&#39;s defensive pressure characteristics, which we narrowed to pressure success rate and proportion of pressure in the attacking third, and the points earned by the team. We now do the same for the GA and SoTA team metrics. To begin with, let&#39;s look at the FBref statistics are in the table below: . Observations: . Manchester City has the fewest GA (32) and by far the fewset SoTA (168). | Sheffield United have the 4th highest GA (53) and the scond-highest SoTA (205) | . Relating these statistics with the previous section we, see that the team with be best GA and SoTA preformance (Man City) produced the fewest pressure events, but also had the highest success rate and one of the highest proportions of pressures in the attacking third. Conversely, Sheffield United produced the most pressure events, but with low success and more of the events in its own third. while also having some of the worst GA and SoTA metrics. Therefore, it is not the raw number of pressures that makes a difference, but how the pressure is applied: the success rate and, to a lesser but still significant degree, the location of the pressure. . Indeed the plots below and the corresponding correlation coefficients show a strong negative correlation between GA and SoTA and the success rate and the location of the pressure events: . Pressure success % Attcking third pressure % . GA | -.56 | -.47 | . SoTA | -.44 | -.63 | . GA and SoTA are more closely related to the pressure characteristics than points, because they measure quantities more directly dependent on how the team plays: they are closer to the dynamics of the game. The above results also make intuitive sense: the higher the team&#39;s pressure, the less opportunities the opponent will have to take shots on goal, because the opponent is forced to expend effort in their own half and work around the press, away from the team&#39;s goal. On the other hand, while effective at reducing SoTA, a high press may not necessarily result in winning possession, so the success rate becomes more important as it relates to GA. . These results, together witht he comments made in the prevous section, explain the difference in performance between Manchester City and Sheffield United. . fig, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(20,10)) subplot_squads(all_data, x=&#39;%&#39;, y=&#39;GA&#39;, ylabel=&#39;GA&#39;, xlabel=&#39;Percent of successful pressures&#39;, ax=ax1) subplot_squads(all_data, x=&#39;Att 3rd %&#39;, y=&#39;GA&#39;, xlabel=&#39;Percent of pressures in attacking third&#39;, ax=ax2) . ga_att_pct_corr = all_data[&#39;GA&#39;].corr(all_data[&#39;Att 3rd %&#39;]) ga_suc_pct_corr = all_data[&#39;GA&#39;].corr(all_data[&#39;%&#39;]) print(&#39;Correlation between GA and success rate of pressures: {:f}&#39;.format(ga_att_pct_corr)) print(&#39;Correlation between GA and percent of pressure in the attacking third: {:f}&#39;.format(ga_suc_pct_corr)) . Correlation between GA and success rate of pressures: -0.563937 Correlation between GA and percent of pressure in the attacking third: -0.465390 . fig, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(20,10)) subplot_squads(all_data, x=&#39;%&#39;, y=&#39;SoTA&#39;, ylabel=&#39;SoTA&#39;, xlabel=&#39;Percent of successful pressures&#39;, ax=ax1) subplot_squads(all_data, x=&#39;Att 3rd %&#39;, y=&#39;SoTA&#39;, xlabel=&#39;Percent of pressures in attacking third&#39;, ax=ax2) . sota_att_pct_corr = all_data[&#39;SoTA&#39;].corr(all_data[&#39;Att 3rd %&#39;]) sota_suc_pct_corr = all_data[&#39;SoTA&#39;].corr(all_data[&#39;%&#39;]) print(&#39;Correlation between SoTA and success rate of pressures: {:f}&#39;.format(sota_att_pct_corr)) print(&#39;Correlation between SoTA and percent of pressure in the attacking third: {:f}&#39;.format(sota_suc_pct_corr)) . Correlation between SoTA and success rate of pressures: -0.440233 Correlation between SoTA and percent of pressure in the attacking third: -0.633339 . Discussion . I repeat here the summary from the introduction, and conclude with a paragraph on choosing the right metrics. . In this activity I explore the relationship between team performance and team pressure behavior using data from the English Premiere League season 2020-2021. I focus on Manchester City and Sheffield United, respectively the winner and bottom finisher for that season. I describe how points, goals-against (GA) and shots-on-target-against (SoTA) are related to the success rate and the field location of teams&#39; pressure acvitivies, and how these relationships explai the different final placements achieved by high-perfoming and low-performing teams. The analyzed metrics are related to passes-per-defensive action (PPDA), a summary metric of a team&#39;s efficiency in applying defensive pressure, but in this note I do not look at this metric. Instead I focus on readily available, first-principles metrics that are nonetheless informative with only a minimum of data processing. While my observations are qualitative, they are supported by quantitative and exploratory analyses performed in Python. . While high-press (applying defensive pressure in the opponent&#39;s third of the pitch) is not required for teams to win games, there exist a correlation between winning (points achieved) and using high-press. However, it is not sufficient to simply apply pressure (proximity within 5 meters of an opponent with the ball) in quantity in the attacking third: the quality of the pressure is also important, as measured by the success rate of pressure events (success is winning possession). Furthermore, it is important to also look at the normalized profiles of pressure events across the thirds of the pitch to characterize how a team chooses to distribute its totall pressure effort throughout a match in the three zones: raw counts, even segmented by pitch thirds, are misleading. . For the purposes of analysing a team&#39;s pressure effectiveness, metrics closer to the played game such as GA and SoTA are more relevant than points earned. For these metrics there is a stronger correlation with the success rate of pressure and the proportion of pressure applied in the attacking third. . The statistics of Sheffield United (bottom-finisher) and Manchester City (EPL winner), are surprising at first if we only consider points won. Sheffield produced the highest number of pressures, both in total and in the attacking third, but finished last. Manchester City, conversely, had the fewest number of pressures, yet was the winner. The distinguishing characteristic for Man City is the highest success rate of the pressures and its stronger propensity toward the high-press. . Success in pressure depends on many factors. It is not limited to the individual pressure event that leads to winning possession, but it also depends on the teammates covering the spaces or marking 1:1 correctly, applying their chosen pressure structures correctly, being ready on their jumps-to-pressure, and on any dynamic advantage of the individual players. Manchester City possesses more of these team and individual characteristics than Sheffield United and it is likely better able to choose the moments for applying pressure, increasing the efficiency of pressure activities. This, together with the fact that they practice a higher press, contributes to their low GA and SoTA metrics. . Finally, it is important to distinguish overall win/lose-type of statistics, from more descriptive statistics. While a team&#39;s overall objective is to win, the objective of a tactical/match analyst is to understand their own team and the opponents: simple winning statistics are not as helpful here. Instead, choosing metrics more closely connected to the actions taken on the field, like GA and SoTA in this case, yields deeper undertanding and more fruitful exchange of ideas. . License You may use and modify this Jupyter notebook under the terms of the BSD license. .",
            "url": "https://lucacazzanti.github.io/abacus-canvas/soccer%20analytics/2022/01/14/High_Press_in_Football_Activity4.html",
            "relUrl": "/soccer%20analytics/2022/01/14/High_Press_in_Football_Activity4.html",
            "date": " • Jan 14, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Confidence Intervals for Gamma Distribution Parameters",
            "content": "Summary . I use visualization and bootstrap to obtain empirical confidence intervals for the estimates of the $k$ parameter of a Gamma distribution. I consider the dependency of the estimates on the sample size and on the true value of $k$, and the differences in the estimates computed with the method-of-moment (MoM) and maximum likelihood estimation (MLE). The results are in line with what we already know from statistics: . The higher sample size, the higher the achievable confidence for a given tolerance on the $k$ estimates. | MLE gives (slightly) more robust estimates than MoM across values of $k$ and across sample sizes. | I also quantify (approximately) the relationship between the confidence level and the fractional error in the estimates for $k$: At the 95% confidence level, estimates for $k$ deviate from the true value by $ pm$28% (sample size=100), by $ pm$12% (sample size=500), and by $ pm$9% (sample size=1000). These results suggest a minimum sample size in the range 100-500 for reasonable estimates in practical settings, although the acceptable deviation from true $k$ depends on one&#39;s application. . Tools used: the seaborn statistical visualization module, scipy, pandas, in addition to matplotlib. . The Gamma distribution: the basics . The Gamma distribution depends on two parameters, $k$ and $ theta$, and looks like this: . import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns import pandas as pd from scipy.stats import gamma import numpy as np . . def make_gammas(ks, thetas, x): &quot;&quot;&quot; Make a dataframe of Gamma PDFs. Args: ks (1d array-like): values of k thetas (1d array-like): values of theta x (1d array-like): points at which to evaluate the Gamma pdf. Returns: Pandas dataframe where eah column contains the Gamma pdf evaluated at x for each pairwise combination of the parameters in ks and thetas. &quot;&quot;&quot; res = {} ordered_keys = [] for theta in sorted(thetas): for k in sorted(ks): name = &#39;$k$=&#39; + str(k) + &#39;; $ theta$=&#39; + str(theta) res[name] = gamma.pdf(x, k, scale=theta) ordered_keys.append(name) return pd.DataFrame(res)[ordered_keys] . . gamma_df_k = make_gammas([1,2,3,5,7,9], [2], np.linspace(0,25, num=100)) gamma_df_theta = make_gammas([2],[1,2,5,7,11,13], np.linspace(0,25, num=100)) fig, axarr = plt.subplots(1,2,figsize=(15,5), sharey=True) gamma_df_k.plot(ax=axarr[0], fontsize=16) axarr[0].legend(fontsize=14) gamma_df_theta.plot(ax=axarr[1], fontsize=16) axarr[1].legend(fontsize=14) plt.suptitle(&#39;Gamma distributions for various $k$ and $ theta$ values&#39;, fontsize=18) . . &lt;matplotlib.text.Text at 0x7f77c7214590&gt; . From the above plots we see that both $k$ and $ theta$ affect the look of the Gamma pdf. The Gamma pdf is skewed to the left, with a longer tail on the right side of the mode. For $k$=1, the Gamma pdf is simply an exponential pdf (maximum asymmetry) and for $k &gt; approx$9 the pdf approaches a Gaussian (symmetry). Lower values of $ theta$ squish the distribution horizontally toward the left; higher values stretch it and flatten it, but the overall profile is not affected. For these reasons $k$ is called the shape parameter and $ theta$ the scale parameter. . The parameters $k$ and $ theta$ are related to each other through the mean and variance of the Gamma distribution: $$ mu = k theta, $$ $$ sigma^2=k theta^2.$$ Thus, given the mean, we only need to estimate one parameter and then compute the other from the estimate. The scipy.stats.gamma.fit() implementation lets you choose which parameter to estimate and which one to constrain: I chose to estimate $k$ and derive $ theta$. Finally, since the Gamma pdf increasingly resembles a Gaussian for higher values of $k$, here I limit my exploration to $k lt 10$. For higher values a Gaussian might be a better model, or one could shift the data x down (to the left) so that min(x)=0 and assume lower values of $k$ for approximating the shifted pdf. Either way, I look only at $k lt10$. . Estimating k in practice and confidence intervals . In pratical settings, a limited number of samples are available, which introduces uncertainty about the estimated value of $k$. We know from probability theory that the uncertainty decreases as the number $n$ of available samples increases: in fact, the uncertainty is inversely proportional to $ sqrt{n}$. One can measure this effect empirically by repeatedly estimating $k$ many times for different sample sizes, and observing the resulting sampling distribution: this is called bootstrap, and it&#39;s the method I adopt here. . With the bootstrap, one can compute the confidence intervals around the estimates. Confidence intervals are simple plus-or-minus bounds associated with the estimated value and with a level of confidence, which is simply the probability that the estimated $k$ will fall within the bounds. Typically folks aim for a 95% confidence interval: what are the bounds within which the true value of $k$ will fall with a probability of 95%? Or, you can turn around the question and ask: Given that I can only tolerate a given bound on the estimation error, what is the probability (confidence) that the estimate will fall within those bounds? . But how do we estimate $k$? There are two gneral approaches from statistical learning that we can apply here: the MoM and MLE. . MoM: Method of moments . With MoM, one sets the empirical moments (mean, variance etc.) to their theoretical expression in terms of the distribution parameters, then solves for the parameters. For a Gamma distribution we know that the mean $ mu=k theta$ and the variance $ sigma^2=k theta^2$. So, taking the values of $ mu$ and $ sigma^2$ from the sample, the closed-form solution for $k$ and $ theta$ is: $$k = frac{ mu^2}{ sigma^2},$$ $$ theta = frac{ sigma^2}{ mu}$$ . Here&#39;s a trivial Python implementation: . import numpy as np def gamma_mom(x): &quot;&quot;&quot; Estimate the parameters of a Gamma distribution using the method-of-moments. Args: x (1d array-like): Data, assumed drawn from a Gamma distribution. Returns: (k, theta) parameters &quot;&quot;&quot; avg = np.mean(x) var = np.var(x) k = avg**2 / var theta = var / avg return k, theta . MLE: Maximum likelihood estimation . With MLE, one maximizes the likelihood function of the observed the data over the Gamma parameters. For Gamma, there is no closed-form expression for the maximum as a function of the parameters, so we must resort to numerical methods. Luckily scipy.stats.gamma.fit() implements MLE for Gamma distributions for us, based on work by Choi and Wette and Minka. . MoM and MLE: single $k$ . Lets calibrate the discussion to what we already know. For a single value of $k$, let&#39;s generate many bootstrap draws the corresponding gamma and compare the sampling distributions for the estimates obtained with the MoM and the MLE. . import itertools def gamma_bootstrap_estimate(true_k, true_theta, sample_size=[50, 100, 1000], draws=100, method=&#39;all&#39;): &quot;&quot;&quot; Estimate the parameters of a Gamma distribution with bootstrap. Generates bootstrap samples and estimates the (k, theta) parameters from each draw, using the method-of-moments and scipy.stats.gamma.fit Args: true_k (scalar): the k parameter of the Gamma distribution. true_theta (scalar): the theta parameter of the Gamma distribution. sample_size (1d array-like of ints): the number of random samples to draw from the Gamma distribution for each bootstrap draw. draws (int): number of bootstrap draws (100) method (str): one of &#39;mom&#39;, &#39;scipy&#39;, or &#39;all&#39; (&#39;all&#39;) Returns: A pandas DataFrame where each row contains the estimates for k and theta for one bootstrap draw. &quot;&quot;&quot; true_mean = true_k * true_theta true_var = true_k * true_theta ** 2 result = [] for this_many in sample_size: # Generate this_many samples from the true Gamma rvs = [ gamma.rvs(true_k, scale=true_theta, size=this_many) for n in xrange(draws) ] if method == &#39;all&#39; or method == &#39;scipy&#39;: estimates_scipy = ( gamma.fit(x,floc=0) for x in rvs ) (k_scipy, loc_scipy, theta_scipy) = itertools.izip(*estimates_scipy) result.append({&#39;sample_size&#39;: this_many, &#39;k_estimate&#39;: k_scipy, &#39;theta_estimate&#39;: theta_scipy, &#39;true_theta&#39;: true_theta, &#39;true_k&#39;: true_k, &#39;method&#39;: &#39;scipy&#39;}) if method == &#39;all&#39; or method == &#39;mom&#39;: estimates_mom = (gamma_mom(x) for x in rvs) (k_mom, theta_mom) = itertools.izip(*estimates_mom) result.append({&#39;sample_size&#39;: this_many, &#39;k_estimate&#39;: k_mom, &#39;theta_estimate&#39;: theta_mom, &#39;true_theta&#39;: true_theta, &#39;true_k&#39;: true_k, &#39;method&#39;: &#39;mom&#39;}) return pd.concat( [pd.DataFrame(r) for r in result]) . true_k = 2 true_theta = 2 num_samples = [10, 25, 50, 100, 500, 1000] num_draws = 1000 estimates_for_one_k = gamma_bootstrap_estimate(true_k, true_theta, num_samples, draws=num_draws) estimates_for_one_k.head() . k_estimate method sample_size theta_estimate true_k true_theta . 0 3.465786 | scipy | 10 | 0.882095 | 2 | 2 | . 1 5.106975 | scipy | 10 | 0.797050 | 2 | 2 | . 2 3.020494 | scipy | 10 | 1.002957 | 2 | 2 | . 3 4.314278 | scipy | 10 | 1.233048 | 2 | 2 | . 4 2.175172 | scipy | 10 | 1.090711 | 2 | 2 | . The violin plot is a graphical data analysis method for direct comparison of two distributions across different values of a grouping variable (number of samples in our case). Let&#39;s take a look: . import seaborn as sns sns.set(style=&quot;ticks&quot;) sns.set_context(&quot;poster&quot;) # this helps when converting to static html for blog plt.figure(figsize=(15,7)) ax = sns.violinplot(x=&#39;sample_size&#39;, y=&#39;k_estimate&#39;, data=estimates_for_one_k, hue=&#39;method&#39;, palette=&#39;muted&#39;, inner=&#39;quartile&#39;, split=True, hue_order=[&#39;mom&#39;, &#39;scipy&#39;], linewidth=1) sns.despine(offset=10, trim=True) title_str = &#39;Estimates of k from &#39; + str(num_draws) + &#39; bootstrap draws; true k=&#39; + str(true_k) + &#39;, true $ theta=$&#39; + str(true_theta) plt.title(title_str) . &lt;matplotlib.text.Text at 0x7f77c717e210&gt; . Observations on the violin plot . I see that for both MoM and MLE the sampling distribution spread decreases as the number of samples increases. We knew his from the central limit theorem (CLM). I also see that the spread of the sampling distribution for the MoM estimates is a bit wider than for the MLE, which suggests that MLE could buy you a bit more confidence in the estimates. Next I want to look at these differences in spread more closely and test these observations for different values of $k$. . MoM and MLE: many $k$s . To compare the estimates across $k$, I normalize the error to a fraction between -1 and 1. I stop at k=9 because for higher values the Gamma distribution starts to look like a Gaussian, and a Gaussian model might be sufficient in practical settings. I visualize the results in panels of boxplots, which help hone in on the quartiles of a distribution. . df_list = [] theta_val = 2 for k in [1,2,3,5,7,9]: tmp = gamma_bootstrap_estimate(k,theta_val, sample_size=num_samples, draws=num_draws) df_list.append(tmp) big_df = pd.concat(df_list) big_df[&#39;fractional_error&#39;] = (big_df[&#39;k_estimate&#39;] - big_df[&#39;true_k&#39;] ) / big_df[&#39;true_k&#39;] big_df.head() . k_estimate method sample_size theta_estimate true_k true_theta fractional_error . 0 1.229787 | scipy | 10 | 1.059610 | 1 | 2 | 0.229787 | . 1 0.790053 | scipy | 10 | 3.453050 | 1 | 2 | -0.209947 | . 2 0.975968 | scipy | 10 | 1.461941 | 1 | 2 | -0.024032 | . 3 1.004771 | scipy | 10 | 1.284221 | 1 | 2 | 0.004771 | . 4 0.754723 | scipy | 10 | 1.668499 | 1 | 2 | -0.245277 | . # make and adjust the subplots manually... true_k = big_df[&#39;true_k&#39;].unique() num_k = len(true_k) ncol=3 nrow= num_k / ncol sns.set(style=&quot;ticks&quot;) sns.set_context(&quot;poster&quot;) f, axarr = plt.subplots(nrow, ncol, sharex=True, sharey=True, figsize=(15,14)) for row in range(nrow): for col in range(ncol): idx = row * ncol + col this_ax = axarr[row,col] sns.boxplot(ax=this_ax, x=&quot;sample_size&quot;, y=&quot;fractional_error&quot;, hue=&#39;method&#39;, hue_order=[&#39;mom&#39;, &#39;scipy&#39;], data=big_df[big_df[&#39;true_k&#39;] == true_k[idx] ], palette=&quot;muted&quot;, showfliers=False, linewidth=1, showmeans=True, meanline=True) this_ax.set_title(&#39;k=&#39;+str(true_k[idx])) if row == 0: this_ax.set_xlabel(&#39;&#39;) if col &gt; 0: this_ax.set_ylabel(&#39;&#39;) sns.despine(offset=10, trim=True) plt.subplots_adjust(wspace=0.4, hspace=0.3) plt.suptitle(&#39;Fractional estimation error across k, for $ theta$=&#39;+str(theta_val)) . &lt;matplotlib.text.Text at 0x7f77c459d0d0&gt; . Observations on the box plots . In the above grid of box plots I notice the following: . It seems that the MLE and MoM are not dependent on the true value of $k$, except that for $k=1$, for which the Gamma becomes an exponetial pdf, the MoM gives visually noticeable higher error. | MLE gives more robust estimates, which show up as smaller inter-quantile ranges (IQRs, the upper and lower box edges) | The median and mean fractional error (black and red lines, respectively, inside the boxes) for both MLE and MoM are consistently greater than 0, which tells me that the sampling distribution is skewed. This effect is particularly noticeable for sample sizes 10 and 25. Mean and median converge toward each other and toward 0 for larger sample sizes, which means that the sampling distributions become more symmetric. We observed this in the violin plots above, too, and it is expected by the central limit theorem (CLT). | Confidence intervals . I want to eastimate confidence intervals, so I compute quantiles of the estimates of $k$. Again, to facilitate the comparison across different true values of $k$ and across sample sizes I adopt the fractional error metric, but I take its absolute value to simplify the visual exploration of the charts below. In this case taking the absolute value is like assuming that the fractional error is symmetric around 0. We have seen above that this is not the case for small sample sizes, but for sample size 100 and greater it is more plausible. Nonetheless, for the purpose of this blog, please allow me to simplify in this way. . Finally, to avoid visual information overload, I do not color-code the different values of $k$ in the quantile plots below. Instead, estimates for different $k$s are color-coded by estimation method, so the plots will display 6 markers per method per confidence level, color-coded by MoM or MLE. This visualization is clearer and consistent with the observation that the true value of $k$ has a smaller effect on the estimates than the sample size. Here we go! . q = [0.05,0.1,0.2,0.3, 0.4,0.5,0.6,0.7,0.8,0.9, 0.95, 0.975] #quantiles big_df[&#39;abs_fractional_error&#39;] = big_df[&#39;fractional_error&#39;].abs() grouped = big_df.groupby([&#39;method&#39;, &#39;sample_size&#39;, &#39;true_k&#39;]) grouped_quantiles = grouped[&#39;abs_fractional_error&#39;].quantile(q) grouped_quantiles.name=&#39;fractional_error_quantile&#39; # Give a name to the quantile level column grouped_quantiles.index.names = map(lambda n: n if n is not None else &#39;confidence_level&#39;, grouped_quantiles.index.names) # Flatten the results for easier plotting: Took me a while poking around # the pandas doc to find this way to flatten the multi-index created by groupby() above! quantiles_df = pd.DataFrame( pd.DataFrame(grouped_quantiles).to_records() ) quantiles_df.head() . method sample_size true_k confidence_level fractional_error_quantile . 0 mom | 10 | 1 | 0.05 | 0.034163 | . 1 mom | 10 | 1 | 0.10 | 0.062888 | . 2 mom | 10 | 1 | 0.20 | 0.132973 | . 3 mom | 10 | 1 | 0.30 | 0.211379 | . 4 mom | 10 | 1 | 0.40 | 0.312867 | . sns.set(style=&quot;darkgrid&quot;) sns.set_context(&quot;poster&quot;) f = plt.figure(figsize=(17,15)) g = sns.FacetGrid(quantiles_df, despine=True, sharey=False, col_wrap=3, col=&#39;sample_size&#39;, size=4, legend_out=True, hue=&#39;method&#39;, hue_order=[&#39;mom&#39;, &#39;scipy&#39;], margin_titles=True, palette=&#39;muted&#39;, xlim=[-0.1, 1.1]) g.map(plt.scatter, &quot;confidence_level&quot;, &quot;fractional_error_quantile&quot;) # Adjust ylims and plot vertical line at a given confidence level conf_level = 0.95 for ax in g.axes.flat: ylims = ax.get_ylim() new_ylims = [-0.01, ylims[1]] ax.set_ylim(new_ylims) ax.plot([conf_level, conf_level], new_ylims, &#39;r--&#39;, alpha=0.5, linewidth=1) sns.despine(offset=10, trim=True) g.add_legend() g.fig.subplots_adjust(wspace=0.4, hspace=0.5); . &lt;matplotlib.figure.Figure at 0x7f77c4203050&gt; . Observations on the quantile plots . Our previous observation that MLE gives more robust estimates than MoM across sample sizes and values of $k$ is confirmed: MLE depends less on the value of $k$ (tighter green vertical spreads) and, for equal confidence level, gives less fractional error in the estimated $k$ (green plots consistently lower than blue plots). | For sample sizes &lt;=50, at a 95% confidence level (red vertical line) you&#39;d need to tolerate uncertainties in $k$ between about $ pm$ 40% to $ pm$ 200%. Perhaps your application can deal with this range, perhaps not. | For sample sizes &gt;= 100, the confidence intervals become more reasonable: for 500 samples the 95% confidence interval is about $ pm$ 12% for MLE, and about $ pm9$% for 1000 samples. | Conclusions . I explored the link between sampling size, Gamma distribution parameters, and estimation methods using the bootstrap and visualization. What I found is well known, not just for Gamma distributions: sampling effects diminish as the sample size increases and different estimation methods give different results. I was also interested in calculating confidence intervals on the estimates that I could use a guidelines in the future. I now know, for example, that if I want my estimate of $k$ to be within $ pm$15% of true value 95% of the times, I need about 500 samples. In fact, I suggest two guidelines: . Prefer MLE over MoM for Gamma distributions because it buys you a bit more confidence. | Prefer sample sizes &gt; 100, but 1000, or even 500 may not be necessary, depending on the requirements of your application. | Relevance: why does it matter? . Sampling introduces uncertainty in addition to the randomness of your chosen probability model. You should have an idea about this additional uncertainty, because it affects the predictions downstream of the estimation and therefore your key performance indicators (KPIs). In a typical data product (recommendation engine, personalized marketing offer, etc.), you have an overall error budget dictated by the allowed tolerances of your problem, and you allocate that error budget to the various steps in your data processing pipeline. For example, say your KPI is average revenue per user (ARPU) and you must estimate it within $ pm5%$, then all steps in your pipeline must guarantee that you will stay within that limit. . It&#39;s true that big data give us the luxury of huge sample sizes, compressing the uncertainty down to unreasonably effective ranges, and making sampling effects essentially insignificant. But even when you have big data sets available, you may need to partition it into many, disjoint subsets to run hundreds or thousands of independent experiments, each one requiring some sort of parameter estimation. This is the case of mass personalization, for example, in which finely targeted marketing offers are presented to smallish groups of customers. Determining the appropirate offer requires running many A/B tests, which means reserving target and control groups for each test. If each one requires 100 customers and you have 100 offers to test, then you need 10,000 customers set aside. If you are Google or Facebook that&#39;s peanuts, but if you are a startup you may not have that meny customers to experiment with. So, You should strive to minimize the number of samples (in this case customers) needed for eaxh experiment, which means that you must know the additional uncertainty you introduce when you make your sample sizes smaller. This will help you consider more carefully how the uncertainty propagates through the steps of your data analysis and how it affects your KPIs. . License You may use and modify this Jupyter notebook under the terms of the BSD license. .",
            "url": "https://lucacazzanti.github.io/abacus-canvas/statistics/estimation/gamma/2016/05/29/EstimatingGammaParameters.html",
            "relUrl": "/statistics/estimation/gamma/2016/05/29/EstimatingGammaParameters.html",
            "date": " • May 29, 2016"
        }
        
    
  
    
        ,"post3": {
            "title": "Know your questions!",
            "content": "Know your questions! . . Articulating precisely the question you are asking of the data gives clarity and focus to your data analysis. A complication is that often a data analysis flows through different stages in a non-linear way: you have some preliminary assumptions, you explore the data, you revise the assumptions, then notice a correlation between variables, go on a week-long deep-dive to investigate it until you realize it was nothing, and so on and so forth. Another complication is that, as you work through the stages of the analysis, you sometimes must revise or completely change the original, broader question based on what you are learning. Managing these challenges is essential to producing an impactful analysis that answers the right question and abides by the best practices. . Over the years I’ve found that being aware of the type of question I am asking at each stage of a data analysis helps me work through these challenges. I’ve also found that being deliberate about noticing the stage transitions in a data analysis project helps me avoid time-consuming detours and dead-ends. The key is to label each data analysis stage with the appropriate type of question. So, what are the types of questions? For a long time I did not have a crisp nomenclature to help me, so I followed my intuition and experience, with mixed success. Recently I came across the work of researchers Roger Peng, Elizabeth Matsui, and Jeff Leek who have come up with a crisp list of six types of questions relevant to data analysis: descriptive, exploratory, inferential, predictive, causal, and mechanistic. I now use their list to guide me through my data analyses. Below, I summarize the six types of questions and riff on them a bit with my own observations from my data analysis experiences. . The Six Types of Questions, according to Peng, Matsui, and Leek. . Peng, Matsui and Leek describe six types of questions for data analysis: . Descriptive . In a descriptive data analysis, all you do is describe the data. Typical outputs of these analyses are simple aggregations, like counts, percentages, bar charts, probability density estimates, quantiles, maximum, minimum, and missing values. You do not attempt to explain or interpret these summary statistics, or relate them to other sources of information. Dashboards are an example output of a descriptive data analysis: they summarize the data and offer it up to human experts for interpretation. . Almost always, a descriptive analysis is my first step in a broader study/analysis. It gives me an idea of the data size, distribution, and types, and helps me form a first guess at the types of analyses achievable with the given the data set. In fact I would say that every data analysis should go through a descriptive stage, even if its scope is limited. If you jump straight to inference or prediction without describing the data, you may not be able to interpret your results, or, horrors, may draw the wrong conclusion. So do yourself a favor: describe your data first. There are even built-in tools in Python: pandas.DataFrame.describe() and Dato’s graphlab.SFrame.show() are good places to start, or fire up Tableau. There are no excuses! . Exploratory . Think of exploratory data analysis as the idea-generation phase of a study. During this phase you formulate hypotheses about the relationships between subsets of the data. Perhaps you notice that shoppers tend to buy more hot dogs and beer on warm weekends; perhaps you notice an increased presence of pleasure boats during the summer on a given waterway; or maybe you observe that some of the 1000s of time series in your data set could be grouped into a small number of clusters. Whatever the subject matter, I find the exploratory phase very fun because it gives me creative license to free-associate, cut, slice, transform, and visualize the data, and formulate and quickly de-risk –sometimes improbable– hypotheses. I experience a sense of excitement and of possibility that stimulates my creativity. . With all this excitement it’s easy to get carried away and mistake a generated hypothesis for a conclusion. Don’t do it! Instead, make a list of the hypotheses, and support each of them with charts and quick calculations. You can do this easily with the Jupyter notebook in Python or with Tableau. Then figure out which one to address further first, perhaps with the help of teammates or subject matter experts. This is a perfect opportunity to share your hypotheses with your data analysis team, which may include other data scientists, subject matter experts and business stakeholders. Careful though: make sure you manage the message if your audience is not familiar with data analysis workflows. You are still at the rough draft stage of your overall analysis and you don’t want your preliminary hypotheses to be misconstrued as conclusions by an eager, but not data-savvy, manager. . Inferential . Remember those hypotheses you generated during the exploratory phase? In an inferential analysis you seek to validate a hypothesis on a different data set, or infer that what you observed during the exploratory phase holds in general. Why would you want to do that? Well, think of polls and surveys. The available data are a subset collected from the overall population, but you are seeking to generalize what you learn from the sample to the entire population. Here’s a made-up example. Say you discover that in your data set of 1000 adults, those who exercise at least 30 minutes a day enjoy a 10% decrease in the risk of heart disease. An inferential analysis would tell you if this result holds for the entire adult population, and typically it would also give you a bound on the uncertainty around the result, which captures how robustly the result generalizes to the overall population. . Predictive . Which customers are likely to churn out of a subscription over the next 60 days? Which offer is more likely to entice them to renew their subscriptions? How many cargo vessels will enter the port of Seattle in June 2018? These are examples of predictive questions, which arise in many fields and often underpin entire business models: data-driven startups live or die depending on their ability to answer predictive questions. Sometimes it’s easy to confuse a predictive question with and inferential one. When I’m confused I remind myself that with inference I am testing how well a hypothesis generalizes to a larger, or distinct data set, while with prediction I am attempting to identify the factors that predict an outcome. Here’s another way to remember this: with inference, you are making statements about averages, or other aggregated statistics, over groups; with prediction, you are attempting to predict an individual outcome. . Causal . In a causal analysis, you seek to test a cause-effect relationship between variables. Causation is a stronger relationship than correlation, which merely identifies a link between the variables. Often business problems are answered well enough by a predictive analysis, but sometimes you may be interested in the causes of the relationship between variables, especially when you are attempting to interpret the results more deeply based on the subject matter (with the help of an expert). Just be mindful and do not confuse a successful predictive analysis, which builds on correlations between inputs and outputs, with a causal one. . Mechanistic . A mechanistic question deals with the exact mechanisms involved in a phenomenon. For example, a chemical reaction can be explained exactly and mechanistically in terms of molecular bonds breaking and forming. Other examples are post-hoc reconstruction of events and root-cause analyses. It’s difficult to answer mechanistic questions as part of a prediction or inference task without data from very-well controlled experiments. Frankly, I have never come across a situation that called for a mechanistic data analysis (except for debugging code …), although this reflects my personal experience and is in no way a general statement about data analysis. However, I did catch myself a few times attempting to explain in detail how a variable affected another one even though the analysis did not call for a mechanistic explanation. So, recognize mechanistic thinking when it comes up, and decide if it’s appropriate for the particular stage of the analysis. . Inference, Prediction, Training, and Testing . If you work with big data technologies, you may be able to describe, explore and predict directly on the entire population, without sampling. For example, in a customer churn prediction task, you may have access to the entire customer base, numbering in the millions. So, perhaps you may be tempted to not bother with estimating how well your prediction applies to data outside of the available data set. That would be a mistake. Even if you have access to the entire population at a particular time, the population will change: a customer base changes continuously, new customers arrive and old ones churn out. . Conversely, you may have access only to a very small sample of the overall data set, but need to ensure that your inference extends to the entire population. For inference and prediction then, you need to estimate how well your models generalize. In addition to specifying the sampling process, the standard way to de-risk the generalization is to set aside separate training and testing subsets from the available data. Design your model on the training set, and test it on the testing set. Even better, if your model depends on hyper parameters, you should also have a separate cross-validation data set. In fact, to gain confidence that your prediction will be robust, you should repeatedly and randomly select the training and testing sets and assess the performance on each random split. If the results are consistent across the splits, you are on the right path. . I mention the issue of splitting the data into separate training and testing sets in the context of the types of data analysis questions because it’s a pervasive issue no matter the type of question. It’s possible to be clear about the type of analysis and yet produce incorrect results, unless the data are properly separated in training and testing data. In fact, taking some time to design the training, testing, and cross validation procedures can help you clarify the type of data analysis question you are answering and help you catch mistakes early on. If you are a software developer, a loose comparison to this concept could be test-driven development: think about how you would test your conclusion first, then design your model. . Final remark . I typically find myself working on descriptive, exploratory, inferential, and predictive data analyses. The first two I consider mandatory components of broader data analysis projects, and doing a good job on them gives your project a solid foundation for the subsequent inference and prediction tasks. The latter two require careful consideration of the interplay between the available data, the way it was obtained, and the adopted models. If you are mindful of the type of question you are asking of the data at any one stage and think early about testing your models, your overall data analysis will be better positioned to be correct, relevant, and impactful. . Resources . What is the Question? by Jeff Leek and Roger Peng. | The Art of Data Science by Roger Peng and Elizabeth Matsui. | The Elements of the Data Analytic Style by Jeff Leek. |",
            "url": "https://lucacazzanti.github.io/abacus-canvas/data%20analysis/2016/04/07/data-analysis-questions.html",
            "relUrl": "/data%20analysis/2016/04/07/data-analysis-questions.html",
            "date": " • Apr 7, 2016"
        }
        
    
  
    
        ,"post4": {
            "title": "Merging Sensor Data Streams with Python Generators and Priority Queues",
            "content": "A recurring task in multi-sensor data processing is merging -- or interleaving -- data from multiple sensors while maintaining chronological order. For example, you may be combining temperature readings from different weather stations in a region, stringing together the locations of mobile devices recorded by different cell towers, or piecing together the routes taken by maritime vessels from the data they broadcast through the Automatic Identification System (AIS). . Often, the volume of data recorded by each sensor is large compared to the available RAM; other times each sensor&#39;s data fits in memory, but the merged, sorted stream does not; yet other times the available RAM is insufficient even for smaller sensor data volumes because the merge-and-sort operation is carried out on a small, lightweight compute node (Raspberry Pi anyone?). For these reasons, maintaining order with the naive approach of concatenating-then-sorting-in-memory is infeasible. Fortunately, we can use Python generators, iterators, and priority queues from the Python package heapq to achieve ordered, merged data streams from multiple sensors with low memory overhead. . Motivating use case: Vessel draft as a proxy for loading/unloading activity in ports . . Vessels periodically broadcast their draft -- the vertical distance between the waterline and the vessel&#39;s keel -- through the AIS protocol. Draft readings are used to ensure that loaded vessels to not run aground in shallower waters. Loading and unloading a vessel changes its draft, which can be used as an indicator of progress in port operations and as a proxy for maritime commerce. When multiple AIS data streams report the draft readings, we need to merge them into one unified stream to enable further vessel analytics, such as change detection. . Approach . In this post I assume that there are two AIS sensor networks and data from each are stored in two separte database tables. For simplicity I also assume there are only two vessels. AIS messages transmitted by the vessels are stored in the database tables in the order they arrive, along with a time stamp. In this post I use sqlite3 to simulate this system and use synthetic datasets to illustrate the key points. . To address the requirement to use little memory: use lazy evaluation and generators to iterate over the data streams to avoid creating intermediate data containers. In practice, this means iterating over database connection objects, not over the data in the database. | To address the requirement to keep sorted order: delegate sorting of the individual data streams to the database, then use the powerful heapq.merge() function to interleave the individual streams into one. | . These two techniques have been known to software developers for a long time, but they mey be new to scientists (like me) who do not have formal training in software engineering or databases, but need to analyze large data sets. . Packages and helper functions . import heapq import sqlite3 import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) %matplotlib inline def head_db(table_name, n): &quot;&quot;&quot; Print the first n records from table_name &quot;&quot;&quot; query = &quot;SELECT * FROM %s LIMIT %s&quot; % (table_name, n) conn = sqlite3.connect(&#39;vessel_draft.sqlite&#39;) conn.row_factory = sqlite3.Row result = conn.execute(query) for row in result: res = {key: row[key] for key in row.keys()} print res conn.close() def plot_timeseries(data, **kwargs): &quot;&quot;&quot; Plot the time series of draft readings for one vessel. &quot;&quot;&quot; (timestamp, vessel_id, draft) = zip(*data) plt.step(timestamp, draft, **kwargs) plt.xlabel(&#39;Timestamp&#39;) plt.ylabel(&#39;Draft&#39;) plt.title(&#39;Vessel ID = &#39; + str(vessel_id[0])) plt.ylim([min(draft) - 1, max(draft)+1]) plt.xlim([min(timestamp)-1, max(timestamp)+2]) def plot_timeseries_from_tables(table_names): &quot;&quot;&quot; Plot the time series of draft readings for one vessel from 2 sensors. &quot;&quot;&quot; plt.figure() conn = sqlite3.connect(&#39;vessel_draft.sqlite&#39;) query = &quot;SELECT * FROM %s&quot; % (table_names[0], ) result = conn.execute(query) plot_timeseries(result, where=&#39;post&#39;, marker=&#39;o&#39;, color=&#39;r&#39;, alpha=0.5) plt.hold(True) query = &quot;SELECT * FROM %s &quot; % (table_names[1], ) result = conn.execute(query) plot_timeseries(result, where=&#39;post&#39;, marker=&#39;o&#39;, color=&#39;b&#39;, alpha=0.5) plt.legend([&#39;Sensor 1&#39;, &#39;Sensor 2&#39;], loc=&#39;lower right&#39;) conn.close() def plot_multi_series(data): &quot;&quot;&quot; Plot the time series of draft readings for 2 vessel from 2 sensors. &quot;&quot;&quot; vessel1 = filter(lambda row: row[0] == 1, data) vessel2 = filter(lambda row: row[0] == 2, data) plt.figure(figsize=(10,6)) plt.subplot(121) plot_timeseries(map(lambda row: (row[1], row[0], row[2]), vessel1), where=&#39;post&#39;, marker=&#39;o&#39;, color=&#39;darkviolet&#39;) plt.subplot(122) plot_timeseries(map(lambda row: (row[1], row[0], row[2]), vessel2), where=&#39;post&#39;, marker=&#39;o&#39;, color=&#39;darkviolet&#39;) . Basic scenario: Two sensors and one vessel . Assume there are two sensors (two database files), each one recording the draft of a single vessel, the timestamp of the draft reading and the vessel id. When a new reading is available, each sensor appends the value and the timestamp to its assigned database table. Below is an example of what the data sources might look like. Note that the timestamps recorded by one sensor need not match the timestamps of the other, because the sensors operate asynchronosuly and each one records only the data it observes. This can be seen in the plot: some draft readings are common to the two sensors (violet color), but each sensor has access to readings not available to the other (red and blue color) . head_db(&#39;sensor1_single_vessel&#39;, 7) . {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 1} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 2} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 3} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 6} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 7} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 8} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 11} . head_db(&#39;sensor2_single_vessel&#39;, 7) . {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 1} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 3} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 4} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 6} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 7} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 8} {&#39;draft&#39;: 18.0, &#39;id&#39;: 1, &#39;time&#39;: 9} . plot_timeseries_from_tables([&#39;sensor1_single_vessel&#39;, &#39;sensor2_single_vessel&#39;]) . Let&#39;s join the draft readings from the two sensors into one time series ... . table_names = [&#39;sensor1_single_vessel&#39;, &#39;sensor2_single_vessel&#39;] queries = [&quot;SELECT * FROM %s&quot; % table_name for table_name in table_names] # Open one DB connection per table (sensor) conns = [sqlite3.connect(&#39;vessel_draft.sqlite&#39;) for i in range(len(table_names))] params = zip(conns, queries) readers = [conn.execute(query) for (conn, query) in params] merged = heapq.merge(*readers) plot_timeseries(merged, where=&#39;post&#39;, color=&#39;darkviolet&#39;, marker=&#39;o&#39;) for conn in conns: conn.close() . Voila&#39; a merged series of sensor readings, sorted by time! . Why does this work and why is this efficient? We rely on lazy evaluation: the readers list contains generators over the query results, and the merged result is itself a generator. heapq.merge() does not read the data in memory to sort it. Instead it creates only the internal data structures necessary to index the underlying data, and returns a generator. When one iterates over this generator, the order in which the items are yielded is the desired sorted order over the merged streams. . Let&#39;s take a look at the code in more detail. . Lay out the parameters we will use and open one connection per table (sensor) to the database. We keep the connection objects in the list conns. | . table_names = [&#39;sensor1_single_vessel&#39;, &#39;sensor2_single_vessel&#39;] queries = [&quot;SELECT * FROM %s&quot; % table_name for table_name in table_names] # Open one DB connection per table (sensor) conns = [sqlite3.connect(&#39;vessel_draft.sqlite&#39;) for i in range(len(table_names))] . Set up the queries. The readers list contains iterators over the query results, but the queries are not yet executed. Later, iterating over each reader object will yield a sqlite3.Row object from teh corresponding database table.params = zip(conns, queries) readers = [conn.execute(query) for (conn, query) in params] . | Finally, merge the iterators over the data:merged = heapq.merge(*readers) . Note that merged is itself a generator, thus very little memory is used until it is consumed by the plot function. It is really this simple! | . More realistic scenario - Two sensors and multiple vessels . But wait, there are many vessels, not just one: A vessel draft data stream is more like the one below, where readings for different vessels may arrive at the same time and different vessel ids are interleaved: . head_db(&#39;sensor1_multi_vessel&#39;, 6) . {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 1} {&#39;draft&#39;: 5.0, &#39;id&#39;: 2, &#39;time&#39;: 1} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 2} {&#39;draft&#39;: 7.0, &#39;id&#39;: 2, &#39;time&#39;: 2} {&#39;draft&#39;: 12.0, &#39;id&#39;: 1, &#39;time&#39;: 3} {&#39;draft&#39;: 7.0, &#39;id&#39;: 2, &#39;time&#39;: 3} . This presents a complication: we want to maintain the chronological order of the sensor readings for each vessel, but the data are ordered globally by timestamp because each reading is recorded in the order it arrives, irrespective of the vessel. Feeding sensor tuples as they are to heapq.merge() will produce an undesired result. To address this problem we do the following: . Group together the readings from the same vessels: we can do this simply by modifying the SQL query and use the ORDER BY contruct. This achieves vertical ordering of the readings from each sensor. | Re-order the tuple elements presented to heapq.merge() in the same way as the ORDER BY construct. We do this by declaring explicitly the order of the columns in the SQL SELECT statment. This ensures the correct horizontal ordering for the nested sorted index underlying heapq.merge(). | . table_names = [&#39;sensor1_multi_vessel&#39;, &#39;sensor2_multi_vessel&#39;] # MODIFIED QUERY: nested order by id, time, draft and uses ORDER BY queries = [&quot;SELECT id, time, draft FROM %s ORDER BY id, time&quot; % table_name for table_name in table_names] # Open one DB connection per table (sensor) conns = [sqlite3.connect(&#39;vessel_draft.sqlite&#39;) for i in range(len(table_names))] params = zip(conns, queries) readers = [conn.execute(query) for (conn, query) in params] merged = heapq.merge(*readers) # Store in list for display result = list(merged) for conn in conns: conn.close() . print &quot;(id, time, draft)&quot; for row in result[:10]: print row print &quot;...&quot; for row in result[26:36]: print row plot_multi_series(result) . (id, time, draft) (1, 1, 12.0) (1, 1, 12.0) (1, 2, 12.0) (1, 3, 12.0) (1, 3, 12.0) (1, 4, 12.0) (1, 6, 12.0) (1, 6, 12.0) (1, 7, 18.0) (1, 7, 18.0) ... (2, 1, 5.0) (2, 2, 7.0) (2, 2, 7.0) (2, 3, 7.0) (2, 3, 7.0) (2, 4, 7.0) (2, 4, 7.0) (2, 5, 7.0) (2, 6, 10.0) (2, 7, 10.0) . There it is, a merged stream where the sensor readings for each vessel are ordered by time, and the corresponding plots. . Detecting changes in streaming sensor data . Tipically we are interested in changes in vessel draft, not in the entire sequence of readings, because draft changes slowly compared to the sampling period of the sensor and because changes in values and the time at which they occur carry the key information about port activities and vessel behavior. Furthermore, different sensors may report redundant readings (same value, same timestamp, same vessel id) that we should filter out. We can easily filter out the redundant information and the readings for which the draft has not changed with the powerful unique_justseen recipe, which filters an iterable, keeping an element only if it differs from the previous one. In keeping with our goal of lazy computing, it returns an iterator over the filtered sequence, not the actual data. . I downloaded all the recipes available on the itertools doc page and put them in a module called itertools_recipes. I use the recipes often, as below: . from itertools_recipes import unique_justseen table_names = [&#39;sensor1_multi_vessel&#39;, &#39;sensor2_multi_vessel&#39;] queries = [&quot;SELECT id, time, draft FROM %s ORDER BY id, time&quot; % table_name for table_name in table_names] # Open one DB connection per table (sensor) conns = [sqlite3.connect(&#39;vessel_draft.sqlite&#39;) for i in range(len(table_names))] params = zip(conns, queries) readers = [conn.execute(query) for (conn, query) in params] merged = heapq.merge(*readers) # THIS IS IT -&gt; Change detection change_points = unique_justseen(merged, key=lambda r: (r[0], r[2])) change_pts_result = list(change_points) for conn in conns: conn.close() print &quot;(id, time, draft)&quot; for row in change_pts_result: print row plot_multi_series(change_pts_result) . (id, time, draft) (1, 1, 12.0) (1, 7, 18.0) (1, 15, 15.0) (2, 1, 5.0) (2, 2, 7.0) (2, 6, 10.0) (2, 16, 7.0) . And here they are, merged, parsimonious time series that capture when the draft changes, for two vessel, from two different sensors. . Note that I specified a key for unique_everseen to understand which part of the tuples to use for the queality comparison: I want to determine uniqueness only by the vessel id and draft values (tuple elements 0 and 2). Since the tuples are ordered by time, this results in sorted stream with duplicated draft readings for each vessel filtered out. . Final remarks: scaling, distributing, batch and real-time processing . Don&#39;t let the simplicity of this approach fool you. It is a powerful way to combine many sources of data on small-ish compute nodes and a fundamental building block for more sophisticated analyses. In my work I use versions of it to process $ approx$ 500 million AIS messages from 2-5 sources of AIS data from a SQL Server every month. On my HortonWorks VM running on a quad-core Intel processor, this takes approximately 15 minutes and a small blip in memory usage. It is a perfect match for batch processing of slowly-changing attributes in large datasets and it has been a huge helper for me when I need to prepare data for machine learning algorithms and visualizations. . I deal mostly with historical analyses of data, for which higher latency is acceptable and the entire data over the period of analysis is available. Therefore, delegating the initial sorting and row ordering of the individual streams to the database is feasible and indeed desirable. This approach can be extended to geographically distributed data sources, for example to a scenario where data from hundreds of storage-capable AIS receivers provide the data streams for periodic merging. . I used the term stream to indicate that the data are read in sequence; it is not to be confused with real-time. If you need to merge and sort in real time, then you may need to read small buffers from the data sources in memory and perform the sorting and merging on each buffer within the Python program, accepting that you may obtain only approximate results on time windows larger than the buffer size: speed costs accuracy. Of course, then other frameworks, like Storm and SparkStreaming may be more appropriate, and you may also consider Lambda Architecture in your system design. . Further reading . Sorting 1M 32-bit integers in 2 MB of RAM using Python, by Guido van Rossum | Let your mind be blown by Dave Beazley&#39;s PyCon presentation on Python generators. | Background on AIS | Binary heap on Wikipedia, the data structure that enables the magic in heapq.merge() | . License You may use and modify this Jupyter notebook under the terms of the BSD license. Download the the supporting sqlite database. .",
            "url": "https://lucacazzanti.github.io/abacus-canvas/sensors/streaming%20data/python%20generators/2016/03/19/SensorDataStreamProcessing.html",
            "relUrl": "/sensors/streaming%20data/python%20generators/2016/03/19/SensorDataStreamProcessing.html",
            "date": " • Mar 19, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Luca Cazzanti, Ph.D.",
          "content": "I am a scientist and a technologist with a successful track record in both academic research and product commercialization. I come from a statistical signal processing and data analysis background, which I developed into machine learning and data science expertise applied to diverse fields. I have worked in both large, structured organizations and small, fluid startups, including my own AI consulting outfit. I lead technical teams and communicate with the business units and subject matter experts at the appropriate level of granularity. At the same time, I remain solidly technical and contribute product specifications, system designs, visualizations, exploratory data analyses, prototypes, insights. I most enjoy connecting concepts from machine learning, AI and data science tools to the semantics, constraints, and requirements of specific application domains to deliver solutions that are practical, impactful, and backed by best practices. Visit www.lucacazzanti.net for more information on my past projects, a list of my publications, and my CV, and to get in touch with me. .",
          "url": "https://lucacazzanti.github.io/abacus-canvas/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lucacazzanti.github.io/abacus-canvas/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}